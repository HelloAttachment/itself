\chapter{Methodology}

\section{A Lexical Base Model}
The general model is trained by a single traversal of a large scale corpus.  Token by token, co-occurrence frequencies are tabulated for each word type, and the dimensions of the model are populated with information theoretical statistics measuring the mutual information associated with the observation of a context word occurring within a window of a certain number of words from a vocabulary word.  The result is a matrix with rows corresponding to word-vectors and columns corresponding to co-occurrence terms, such that an element $M_{w,c}$ measures the mutual information inherent in the co-occurrence of term $c$ with word $w$, where $N$ is the total count of word tokens in the corpus, $n_{w,c}$ is the frequency with which $c$ occurs in the context window of $w$, $n_w$ is the total frequency of word $w$, $n_c$ is the total count of times that $c$ is observed in the context of another word, and $a$ is a smoothing constant:

\begin{equation}\label{eq:MI}
M_{w,c} = \log_2 \left(\frac{n_{w,c} \times N}{n_w \times \left(n_c + a\right)} + 1\right)
\end{equation}

Once this base space is established, a dimensional analysis of input terms provides the basis for the projection of contextually informed subspaces.  So, for instance, given an input of terms $w_1$, $w_2$, and $w_3$, the model analyses the corresponding word-vectors $\overrightarrow{w_1}$, $\overrightarrow{w_2}$, and $\overrightarrow{w_3}$ to determine the salient co-occurrence dimensions shared between all three terms, which is to say, those universally non-zero dimensions with the highest mean value.  These dimensions are then used as the basis for the generation of a subspace, with the expectation that conceptually relevant terms will emerge in this new, significantly lower-dimensional space.  In particular, conceptual constituents are identified by searching for word-vectors that cluster around a positive central point considerably removed from the origin, by searching for the word-vectors with the greatest norms, and by searching for the word-vectors that are, on average, furthest away in terms of Euclidean distance from any other word vectors.

\section{Taxonomy Recapitulation}
The conceptual projections of the lexical base model perform particularly well at extending the membership of short lists of words corresponding to constituents of some conceptual category (so, for instance, a list containing \emph{lion}, \emph{tiger}, and \emph{bear} is expanded to include other exemplars of the concept \textsc{wild animal}).  With this in mind, a mechanism for establishing hypernymic relationships between such lists and terms denoting a general categorisation of such lists should lead to the effective reconstruction of hand-crafted lexical ontologies such as WordNet.

The proposed way forward at the time of writing is to establish a technique for analysing the co-occurrence data for singular terms and then to generate short lists of other words as candidates for list expansion operations.  This will most likely involve inspecting potential groupings of the terms that emerge from a subspace for high degrees of mutual co-occurrence, and then expanding top scoring sets to discover potential groupings of terms to be considered as constituents of a certain conceptual subtree.  There will also have to be a system for maintaining and then eventually comparing hypotheses about the conceptual relationships between the denotations of word pairs, as well as a mechanism for ultimately resolving these hypotheses into a well formed ontology.

\section{Analogy Completion}
Where work from researchers such as \cite{Mikolov}, \cite{Pennington}, and \cite{Levy} has focused on building distributional semantic spaces where simple arithmetical operations between word-vectors yield good results for certain types of analogies, the project presented here will focus on more robustly geometric methods to identify congruences between different contextualised projections.  As a first pass at this problem, searching for word-vectors that complete basic geometric transformations within a space seems like a viable approach to this task.  So, for instance, there is some transformation from the space of \textsc{butchery} to the space of \textsc{surgery} that preserves the overlapping co-occurrence relationships between words like \emph{cleaver} and \emph{scalpel} while aligning the 

\section{Metaphor Generation}
The ultimate goal of metaphor generation will involve a kind of reverse engineering of the model's approach to analogy completion: once the mechanism for performing analogical transformations between conceptually rooted word-subspaces has been refined, these transformations can be applied to a given projection based on the dimensional information inherent in the influence of some contextualising term.  Once these transformations are applied, the relatively sparse transformed source space can be searched for the points that best align with the geometry of the target space.
