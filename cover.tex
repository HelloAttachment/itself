
\title{Geometric Methods for Context Sensitive Distributional Semantics}

\author{Stephen McGregor}
\department{School of Electronic Engineering and Computer Science} \college{Queen Mary University of London}
\degree{Doctor of Philosophy} \degreemonth{September} \degreeyear{2017}

%% By default, the thesis will be copyrighted to MIT.  If you need to
%% copyright the thesis to yourself, just specify the `vi' documentstyle
%% option.  If for some reason you want to exactly specify the copyright
%% notice text, you can use the \copyrightnoticetext command.
%\copyrightnoticetext{\copyright ~University of London, 2006}

% The dedication info.
%\dedication{TO MY FAMILY}

% Make the titlepage based on the above information.  If you need something
% special and can't use the standard form, you can specify the exact text of
% the titlepage yourself.  Put it in a titlepage environment and leave blank
% lines where you want vertical space. The spaces will be adjusted to fill
% the entire page. The dotted lines for the signatures are made with the
% \signature command.

% Make the first title page
\maketitle

% make the dedication page
%\makededication

\makedeclaration
\makepublication

% Start to count page number from abstract page
\pagestyle{plain}%
\setcounter{page}{1}
\pagenumbering{roman} %

% The abstractpage environment sets up everything on the page except the
% text itself.
%
% You can either \input (*not* \include) your abstract file, or you can put
% the text of the abstract directly between the \begin{abstract} and
% \end{abstract} commands.
\begin{abstract}
%\input{abstract}
% abstract goes here
This thesis describes a novel methodology, grounded in the distributional semantic paradigm, for building context sensitive models of word meaning, affording an empirical exploration of the relationship between words and concepts. Anchored in theoretical linguistic insight regarding the contextually specified nature of lexical semantics, the work presented here explores a range of techniques for the selection of subspaces of word co-occurrence dimensions based on a statistical analysis of input terms as observed within large-scale textual corpora. The relationships between word-vectors that emerge in the projected subspaces can be analysed in terms of a mapping between their geometric features and their semantic properties. The power of this modelling technique is its ability to generate ad hoc semantic relationships in response to an extemporaneous linguistic or conceptual situation. 

The product of this approach is a generalisable computational linguistic methodology, capable of taking input in various forms, including word groupings and sentential context, and dynamically generating output from a broad base model of word co-occurrence data.  To demonstrate the versatility of the method, this thesis will present competitive empirical results on a range of established natural language tasks including word similarity and relatedness, metaphor and metonymy detection, and analogy completion. A range of techniques will be applied in order to explore the ways in which different aspects of projected geometries can be mapped to different semantic relationships, allowing for the discovery of a range of lexical and conceptual properties for any given input and providing a basis for an empirical exploration of distinctions between the semantic phenomena under analysis. The case made here is that the flexibility of these models and their ability to extend output to evaluations of unattested linguistic relationships constitutes the groundwork for a method for the extrapolation of dynamic conceptual relationships from large-scale textual corpora. 

This method is presented as a complement and a counterpoint to established distributional methods for generating lexically productive word-vectors. Where contemporary vector space models of distributional semantics have almost universally involved either the factorisation of co-occurrence matrices or the incremental learning of abstract representations using neural networks, the approach described in this thesis preserves the connection between the individual dimensions of word-vectors and statistics pertaining to observations in a textual corpus. The hypothesis tested here is that the maintenance of actual, interpretable information about underlying linguistic data allows for the contextual selection of non-normalised subspaces with more nuanced geometric features. In addition to presenting competitive results for various computational linguistic targets, the thesis will suggest that the transparency of its representations indicates scope for the application of this model to various real-world problems where an interpretable relationship betweendata and output is highly desirable. This, finally, demonstrates a way towards the productive application of the theory and philosophy of language to computational linguistic practice.

\end{abstract}

% Acknowledgments
%
% You can either \input (*not* \include) your acknowledgments file, or you can put
% the text of the acknowledgments directly between the \begin{acknowledgments} and
% \end{acknowledgments} commands.
\begin{acknowledgments}
%\input{acknowledgments}
%Acknowledgment goes here...
I have been immensely fortunate to have not just one but two superb supervisors over the course of my studies, and thanks are due to both these gentlemen.  As my first supervisor, Geraint Wiggins has been a source of wisdom, insight, and perpetual encouragement, and I am grateful to have had access to his tremendous enthusiasm for knowledge.  And Matt Purver is officially called my second supervisor, but in fact I've felt as though I have two primary supervisors as Matt has guided me through my studies with vision and patience, sometimes pointing me in the right direction, sometimes feeling out new territory alongside me.  I look forward to being a colleague and friend to both of you in the years ahead.

Beyond that, I am grateful to all my colleagues in the Computational Creativity Lab for the sense of camaraderie that they have brought to my day-to-day life over the past four years, as well as to my friends in the Computational Linguistics Lab, which, under the leadership of Mehrnoosh Sadrzadeh and Matt Purver, has become a cauldron of ideas at the intersection of theory and practice.  Queen Mary's Cognitive Science Group, headed by Pat Healey, has likewise been a vertex of unexpected but productive academic conjunctions, and I have benefited from the cross-disciplinarity that is nurtured there.  And, as my Independent Assessor and also as an occasional debating partner, Graham White has offered a roving perspective on computer science, mathematics, philosophy, and scholarship in general.

Most of all, to mavourneen Nora Rose, you have my eternal adoration.  Thank you for going through this occasionally intense undertaking with me, indeed for occasionally dragging me though it.  May our lives always be so full of events and ideas.

And finally, I must note my gratitude to the Engineering and Physical Sciences Research Council of the UK, who have funded my research through grant EP/L50483X/1.  This is not just a statutory acknowledgement; the Research Councils of the UK are a wonderful resource and a great asset to this country.  It is through their patronage that British universities can continue to be home to the type of innovative, pluralistic research of which I hope this thesis will serve as one small example.  They should be treasured.

\end{acknowledgments}

%\begin{context}
%The term \emph{context} has been used widely and variously by authors in both theoretical and computational linguistics, and with good reason, as various sense of the concept of context are clearly at play in any serious discussion of the interplay between language and cognition.  Statistically minded computational linguists in particular, of whom I would like to count myself as one, have often used \emph{context} to refer to the window of co-occurrence in which a word token is observed within a sample of text.  In his description of a co-occurrence statistic for measuring semantic similarity, \cite{Salton1992b} introduced the term \emph{context space} to refer to a space of co-occurrence dimensions, a terminology subsequently adopted by \cite{BurgessEA1997} in relation to their HAL system.  This notion of proximity within a text as context has persevered in the natural language processing literature.

%Theoretical linguists and cognitive scientists, on the other hand, have tended to treat \emph{context} as a thing 

%So 

%and this nomenclature has been carried on by subsequent researchers interested in the idea that cognition, conceptualisation, and, correspondingly, language are always in some way specified by a situation in the world.

%In this thesis, I will endeavour to use the term \emph{context} strictly in reference to the latter notion of 

%\end{context}

\begin{glossary}
\begin{description}
\item[base space] A high dimensional, sparse vector space of word-vectors, delineated in terms of dimensions of co-occurrence statistics.  \revAK{5}{In many standard distributional semantic applications, the base space is itself a semantic model; in the application outlined in this thesis, the base space will be a generative source of a very high number of contextualised semantic models.}
\item[concept] \revAK{5}{A subject of language and object of cognition.  This essentially contested term points to the substance of thought and at the same time the thing indicated or traced by a representation.  A fundamental commitment of this thesis is that concepts always come about in some context, and that the situational dynamics of conceptualisation are inextricably tied to representation construction.}
\item[context] The situation -- environmental, cognitive, perceptual, linguistic, and otherwise -- in which an agent finds itself and applies language to meaning.  \revAK{5}{This terms is often used in the field of natural language processing to denote a more specific sense of \emph{sentential context} or \emph{co-occurrence context}.  In this thesis, motivated by psycholinguistic and cognitive linguistic literature, \emph{context} will have a more general sense of the overall environment in which cognition and language happen.}
\item[contextual input] A set of words characteristic of a conceptual category or semantic relationship used to generate a subspace for the modelling of semantic phenomena.  \revAK{5}{These words are not in themselves a \emph{context} in the sense defined above; rather they are taken to be indicative of a context.}
\item[co-occurrence] The observation of one word in proximity to another in a corpus.  \revAK{5}{This is sometimes referred to as \emph{context} in the natural language processing literature, but in this thesis I will use \emph{co-occurrence} in order to avoid confusion with the sense of \emph{context} described above.}
\item[co-occurrence statistic] A measure of the tendency for one word to be observed in proximity to another across a corpus.  \revAK{5}{This is sometimes referred to as \emph{weighting} in the natural language processing literature: this statistic is a measure, often with a probabilistic component, of co-occurrence, and is often conceived in a way that offsets mere frequency.}
\item[co-occurrence window] The boundary defining the proximity within which two words are considered to be co-occurring, typically a distance in terms of words within a sentence.  \revJB{12}{A 2x2 word co-occurrence window indicates that the two words to either side of a target word and within the boundary of a sentence will be considered to co-occur with it, for instance.}
\item[dimension selection] The process of contextually choosing a subset of dimensions in order to project a subspace from a base space.  \revAK{5}{This is the process by which a generative base space is translated, through conceptually specified contextual input, into a conceptually specified geometry of lexical semantic representations.}
\item[f-score] \revJB{10}{The harmonic mean of precision and recall, so an effective measure of a well-balanced approach to a classification task.}
\item[meaning] \revAK{5}{The connection between a word, a representation, and a concept.  This tripartite idea of meaning is rooted in the study of semiotics.  This is an unavoidably controversial term, perhaps the most essentially contestable of all the contestable words that come up here, and no definition will satisfy all, or perhaps even most, readers, but it is also essential enough to both the theory and methodology presented here to warrant mention as a keyword.}
\item[methodology] \revAK{5}{Here, t}he process of building base spaces from observations of co-occurrences within a corpus and contextually projecting subspaces through dimension selection.  \revAK{5}{More generally, methodology is taken to refer to theoretically motivated techniques for generating models with the intention of testing likewise theoretically grounded hypotheses.}
\item[model] An application of methodology to a particular linguistic task or experiment, sometimes including task specific statistical analysis techniques.
\item[precision] \revJB{10}{For a classification task seeking to correctly label elements in a dataset, the ratio of correctly applied instances of a given label to the total number of times that label is applied.}
\item[recall] \revJB{10}{For a classification task, the ratio of of instances of a given label which are correctly identified to the total number of elements that should have been assigned that label.}
\item[representation] \revAK{5}{The structural, interactive objects of a conceptual system.  As portrayed here, representations are structurally functional and are labelled by words: these are, then, lexical semantic representations as opposed to what philosophers sometimes refer to as mental representations.}
\item[situation] \revAK{5}{The set of conditions in an environment corresponding to a cognitive context.  This usage has its roots in formal semantics, but the application is motivated more by the philosophy of mind as a feature of environmentally embedded agents.}
\item [subspace] A context specific lower-dimensional projection from a base space, effectively mapping semantic relationships to a context by way of the geometric relationships between word-vectors.
\item[word-vector] A high-dimensional geometrically situated semantic representation of a word, constructed as an array of co-occurrence statistics.
\end{description}
\end{glossary}
