\chapter{Introduction} \label{chap:intro}
``Words,'' writes \cite{Pynchon1973}, ``are only an eye-twitch away from the things they stand for,'' (p. 100).  Words press right up against reality: they are always almost becoming the things that they point at, bleeding into thoughts and actions, taking on shapes or else pressing shapes onto the world of perceptions and experiences that they inhabit.  Words are felt by the ear, on they eye, in the mouth, but also in the mind, on so many levels that the problem of disentangling words from thoughts and meanings has ruined some of the most fastidiously calculated analyses of the nature of cognition and existence.  Language, in its vacillations, becomes so entwined with the way that we encounter reality that it is impossible to extract it without irreparably damaging the boundary between the world itself and the experience of being in the world.  As \cite{Wittgenstein1953} puts it, ``philosophical problems arise when language goes on holiday,'' (\P 38).

In the almost-becoming of language, then, there lurks a treacherous encounter with the inscrutability of having-become---but also an opportunity for an interface with the actual mechanisms of knowing and believing, the exposure of the guts of the apparatus of cognition.  In the very same inescapable closeness of words that has occasionally confounded philosophers, the data-minded scientist might hope to find a conduit for connecting a process of rules and reactions to the murky near-world of signs and meanings.  Words port information from one system to another, traversing the passage from the lived-in world of a communicator to that of a receiver, but there is also information about words, and then, at some point, the information that words carry and the information that carries words bundles into a dynamic semiotic composite, and meaning happens.  One of the principal theoretical commitments of this thesis is that language is in the world: language is experienced materially, and it is the structure of language that, not just in a formal abstraction of syntax but in the way that symbols manifest themselves as components in the entire machinery of causes and intentions, gives words their potency.  So how much can we know about what is in words by knowing about the way that words are in the world?

In the pages that follow, I will describe the theory and application of a novel lexical semantic methodology, predicated on the idea that observations about words as they've been used can lead to a productive model of the relationships between symbols and concepts, implemented through computational processes of word-counting and representation-building geared to map words into a dynamic space of contextually sensitive meaning-bearing structures.  I will demonstrate how these spaces can be generated by an analysis of terms denoting some sort of conceptual continuum, and how they in turn lend themselves to a quantitative, geometric analysis of the relationships between the very words by which they are generated.  This model is built upon a framework of established computational linguistic methodology, and will likewise be tested using data that has been developed and analysed by the natural language processing community.  It also offers an opportunity for applying theoretical insight to quantitative techniques in natural language processing, and, finally, I will argue, a basis for considering ways in which computational models can in turn play a role in subsequent theoretical and philosophical investigations of the nature of language and cognition.

\section{A Question and A Hypothesis}
In my research I have sought to explore the question of the extent to which a data-driven, statistical mechanism, instantiated by an information processing, symbol manipulating machine, can achieve a lexical semantic model that is suited to capturing the protean nature of conceptualisation in a world of unstable and unpredictable situations.  This line of enquiry follows from the idea that cognitive agents are fundamentally enmeshed in their environments, to such an extent that no model of cognition can be abstracted away from a corresponding model of the world without significant loss of efficacy.\footnote{As \cite{Brooks1991} has pointed out, the best model of the world is very often just the world, anyway.}  This supposition presents a serious problem for the computational modelling of semantics, however: how can a machine that is by definition a system of processes unto themselves, with a carefully constrained mechanism for receiving input and offering output, be used to capture the embedded condition of cognition by which semantics arise in the first place?  And here I will refrain from attempting a universal definition of the contentious term \emph{semantics}, but I will broadly apply this word to describe the processes by which symbols or representations that are in some sense tangible commune with the immaterial realm of concepts and meaning.

I will take as a pretence the idea that there are far too many ways to conceptualise, and furthermore that the structures that support conceptualisation are far too complex and varied, to yield to a lexical or conceptual model based on rigid, static symbolic representations, however composite they may potentially be.  Instead, I will seek to build a model which is contextual from the ground up, such that there is no base state that might be construed as standard, default, literal, or in some superlative sense true to a construct of the world as it is---precisely because \emph{the world as it is} is always necessarily just that, an artefact constructed on the premise of some situation determining the units and levels of abstraction on which an analysis is to be performed.  So I propose to seek computational methodologies which are prolific to the point of promiscuity in their capacity for generating conceptual relationships, and here I believe the procedures associated with the machine learning paradigm will in fact prove beneficial: rather than treat the proliferation of data that arises from the analysis of large scale corpora as, as it has sometimes been construed, a \emph{curse}, I will embrace the combinatory immensity of a space of statistics about observations of language as a feature affording perpetual contextualisation.

There is a basic geometric and computational insight to be had here.  In spatial models of semantic relationships, semantics are generally quantified in terms of geometric relationships between the lexical representations projected into the space.  To this well-known approach to semantic modelling I will simply add that geometric measures, when considered as observations from within a system, are relative to the position from which the observations are being made: angles vanish as shapes rotate into a plane that is perpendicular to an observer, and things that are distant from one another can seem close when they are aligned from a certain point of view.  Given interrelated data points in a very high dimensional space, there are necessarily an astronomically large number of lower dimensional perspectives that can be taken on the data; given a choice of perspective, and assuming at least a degree of differentiation in terms of relationships across dimensions, we should be able to arbitrarily select some point of view by which the relationships between data points fall into a desired order.  The trick of modelling semantic relationships in context then becomes the problem of finding a way to reliably select the correct perspective on data without prior recourse to the nature or validity of the affordances of that perspective.  This then gives rise to my fundamental hypothesis:

\begin{quoting}
\noindent In a distributional semantic space defined in terms of dimensions of co-occurrence statistics which are in some sense interpretable, it will be possible project lower dimensional subspaces based on an analysis of input terms in order to generate geometric relationships which can be used to train models to contextually predict semantic relationships.
\end{quoting}

My approach to testing this hypothesis will involve generating base spaces of statistical relationships between words, developing mechanisms for taking lower dimensional perspectives on these base spaces, and then experimenting with the ways that the geometric features of these spaces can serve as input for the supervised learning of linear and logistic models for ranking and classifying semantic phenomena.  Terminologically, I will describe the process of building a base space from the traversal of a corpus and then projecting subspaces from this base space as a \emph{methodology}, in that it is a procedure that is applied to data in response to an input that leads to the output of a new configuration of data supplied for further analysis.  I will then describe the application of machine learning techniques to concatenations of these projected subspaces, or more precisely to matrices of statistics derived from these subspaces, in terms of modelling, and the vectors of coefficients and biases which can be applied to subsequent geometric data will therefore be referred to as \emph{models}.  There is clearly room for variation here: my methodology, the subspaces it dynamically produces, and the feature-weighting models learned from these spaces can all be understood in terms of inputs, parameters, functions, and outputs, but hopefully these terminological commitments will serve to elucidate the descriptions of empirical research in the chapters that follow.

There are two crucial procedural features of my methodology.  The first is the dynamic nature of the projection of contextual subspaces from the base space, which happens in an online way, in reaction to textual input as it arises.  This aspect of the system's architecture has been designed to map, at least on a certain level of abstraction, to the dynamic and lateral nature of a cognitive agent's engagement with an environment, and likewise to the correspondingly productive nature of language by which a staggering multitude of expressions can be generated from a well-defined lexicon.  The second feature is the geometric character of the features that will be mapped from subspaces to models of semantic phenomena.  The process of contextual projection at the core of my approach to distributional semantics facilitates the exposition of semantic relationships as measures that can be lifted directly from the abstract but nonetheless quantitatively palpable environment of a high dimensional space of features that can be interpreted directly in terms of observations in a corpus.  Ultimately, I will make the case that this geometric component of my methodology permits an interpretation, informed by ecological and enactivist approaches to cognition, of meaning as something which is perceived directly in an environment without resorting to a layer of symbolic computation, tying back into the notion of conceptualisation as an emergent property of the dynamics between agent and environment.

A further stipulation of my approach is that my techniques will proceed with minimal recourse to structured information about the relationship between symbols and the things they denote.  This means that I will resist building lexical representations semantically enhanced with information extracted from knowledge bases: using this type of information has often, not surprisingly, proved beneficial in terms of improving scores on data-oriented tasks, but it also muddles the distinction between semantics that have been extrapolated from data versus encyclopaedic knowledge that has simply been successfully transferred from one representational scheme to another.  In fact, I will go even further, avoiding applying any type of dependency parsing or part-of-speech tagging to the corpus from which my representations are extrapolated, instead taking linguistic data as it is discovered \emph{in situ}, with only the barest of assumptions about the boundaries defining words and sentences.  This approach will allow me to abstain from making theoretical commitments to distinctions between syntax and semantics, instead permitting, without necessarily requiring, access to cognitivist theories about the ambiguity between the formal structure of language and the dynamics of conceptualisation.  Adherence to these minimalist principles will mean that I can treat language as a phenomenon encountered physically in an environment, without attachment to any presumptions about the internal architecture of a linguistic agent.

\section{Contributions to the Field}
First and foremost, this thesis presents a novel computational methodology for using linguistic data to generate conceptually productive geometries of word-vectors.  This methodology is grounded in the well known distributional semantic paradigm, which involves the representation of words (or other lexical units) as vectors in high dimensional spaces, constructed on the basis of observations of the way words occur with one another across large scale corpora.  A fundamental characteristic of this approach is that it traffics in lexical representations which are structured in such a way as to be semantically productive: through their relative situation in space, through their composition by linear algebraic operations, and so forth, the representations themselves provide a handle on the way that words become implements of conceptualisation and vessels of meaning.  These representations are constructed through a process of corpus traversal, taking in a very large number of observations about the way in which words tend to co-occur with one another, resulting in a quantitative instantiation of signs as not only the indices but also the operons of meaning-making.  The data-driven nature of this representation-building process means that this technique is naturally amenable to computation, and the advent of massive digitised textual resources combined with the availability of powerful hardware has seen the field flourish in the last several years.

Computers are, however, notoriously literal devices, not, in their application as strictly rule-abiding systems, particularly suited to feeling out the critical nuance that is inherent in human communication, the intransigent looseness between what is said and what is meant.  My contribution to this active area of research is to introduce, by way of a theoretical consideration of the relationship between language and cognition, an element of contextuality to the mechanisms of distributional semantic spaces.  My approach seeks to imbue distributional semantics with an element of interpretability, necessarily relying on statistical spaces which permit a degree of ongoing analysis of situationally generated input in order to perform contextualisations, and in this regard this research is arguably a departure from a trend in computational approaches to building high-dimensional spaces which has tended to embrace the complexity and unknowability of highly non-linear networked processes.  A consequence of my methodology is the projection of subspaces that are rich with axes of geometric relationships: not only are there relationships between points corresponding to words in a subspace, but also between the centre and periphery of the subspace, as well as points pertaining to overall characteristics of the dimensions delineating the subspace.

This makes for a clear point of comparison between my methodology and established natural language processing approaches, which will play out in terms of the comparative results for different models extrapolated from the same underlying corpus on a variety of semantic tasks including word association and similarity ranking, metaphor and semantic type coercion classification, and analogy completion.  In the case of metaphor classification, my results are state-of-the-art, and components of the analogy completion results likewise offer at least a very promising outlook for future exploration.  Elsewhere the results are in many cases competitive, and in all cases provide a valuable basis for a consideration of the special operation of my methodology as well as a reflection on the theoretical assumptions underpinning the model.

It is in this last respect, concerning the theoretical contingencies and consequences of my empirical research, that I envision making my second contribution to the field.  To the extent that the results returned by my methodology can be considered positive, I will make the case that this supports not only the specific hypothesis that the online contextualisation of statistical lexical representations is semantically productive, but also the more general claim that the application of theoretical insight to empirical semantic modelling techniques is worthwhile.  In addition to a general sensitivity to the dialectical and phenomenological schools of philosophy, as well as to the embodied, enactivist trend in cognitive science, I will specifically seek to outline a framework that is informed by two theoretical linguistic approaches, the first being cognitive linguistics and the second being the pragmatically informed relevance theory.  In the first instance, cognitive linguistics has in recent years, thanks to the research of theoretically informed computer scientists such as \cite{Barnden2008} and \cite{ShutovaEA2013}, become a productive background for computational models of semantics and in particular of figurative language.

In the second instance of relevance theory, there has been less contact with computer science, perhaps because there may at first appear to be a disconnect between the drive to model language in terms of stable symbolic structures and the fundamental pragmatic axiom that meaning is always underspecified in the lexicon and only eventually resolved in an online engagement with an environment containing a range of elements which strain the boundaries of symbolic representation.  I will seek to at least begin to redress this disconnect by investigating techniques for extrapolating a range of contextual perspectives on data potentially so large that it seems unbounded: my stance is that computers can in fact be instruments of prolific conceptual vicissitudes and that there are grounds for framing a process of online generation of conceptually productive semantic subspaces in terms of dynamic interaction with an environment at least on a certain level of abstraction.  As such, my methodology has been designed to be at least conversant with the idea that there is really no such thing as a stable conceptual scheme, but rather that concepts are always emerging, unfolding, and then evaporation in an ongoing cycle of representation and interpretation.

\revAK{6/AK-7}{With this said, this thesis belongs very much to the field of computational linguistics, with its methodology and evaluation both grounded squarely in the domain of quantitative approaches to natural language processing.  The contributions outlined in the chapters ahead are to be classified as developments in the domain of computational lexical semantic modelling.  My methodology is rooted in a broad and deep reading of the theoretical and philosophical literature surrounding the study of language and mind, but is somewhat more precise in its focus on questions surrounding computational techniques for representing the meaning and use of words.  Likewise, while there is clear scope for the application of my computational techniques to other fields, ranging from question answering and document retrieval to the burgeoning research area of digital humanities, these applications will only be very briefly sketched in the final pages of this thesis, with further development left for future work.}

\del{With this said, } \revAK{6/AK-7}{So with this combination of broad inspiration and focused application in mind,} I've sought to be open enough in my methodological commitments to permit various theoretical preconditions to and interpretations of the experimental research that I'll describe here.  So, rather than presenting my research as a validation of a particular theoretical stance, I would prefer to more generally suggest that this work is an example of how an empirical project that has been conceived with its philosophical assumptions and implications in mind can become a component in a productive dynamic of theory and practice.  My position is that this theoretically sensitive (as opposed to declarative) approach, coupled with compelling experimental results, offers a platform for a kind of science that can contribute to not just the technological advancement of information processing systems but also offer useful perspectives on the nature of language itself.

\section{Methods} \label{sec:methods}
There are a number of different quantitative techniques invoked throughout this thesis, and, with this in mind, I will offer an overview here rather than repeating introductions to the same methods in various places.  These methods pertain to every stage of my methodology: to the construction of contextually manipulable representations, the projection of context specific subspaces, the measurement of words of interest in these subspaces, the construction of models targeting semantic phenomena based on these measures, and the comparison between different results for a given dataset.  In terms of building representations and selecting contextual projections, my methodology is based on established work in the field but also provides significant new components to the basic distributional semantic approach.  Once sets of geometric features have been established, I apply standard linear and logistic model building operations to these features, and likewise use established quantitative hypothesis testing techniques for the comparison between different results for each task my research pursues.

\paragraph{Representation Construction and Projection} The construction of lexical semantic representations by my methodology employs a basic word-counting strategy, involving the computational traversal of a corpus and the tabulation of co-occurrences that meet an adjustable proximity constraint.  This is described in more detail in Chapter~\ref{sec:pmi}, including an explanation and description of some of the novel enhancements I've applied to a traditional information theoretical approach to calculating word co-occurrence statistics.  The subspace projection technique, which is unique to my methodology, involves an analysis of a set of vectors corresponding to a group of input terms.  This is outlined in detail in Section~\ref{sec:project}.

\paragraph{Task Selection} \revAK{18}{I will use five tasks to experimentally evaluate my methodology, in each case using datasets previously published and analysed in the natural language processing literature.  The tasks will be ranking word-pairs for relatedness, ranking word-pairs for similarity, classifying word-pairs for metaphoricity, classifying word-pairs for semantic type coercion, and completing analogies.  These tasks have been chosen as representative of a range of typical topics in computational semantic modelling, and so will be used to demonstrate the way in which different features of the semantic subspaces projected by my methodology correspond to different semantic phenomena.  The first four tasks are each associated with datasets that are arranged in ways that are conducive to different varieties of statistical modelling, namely linear versus logistic regression, again allowing for exploration of different aspects of my methodology.  And then analogy will be taken as a kind of meta-phenomenon, as various semantic equivalences between linked sets of words can be construed in terms of parallelism across conceptual domains.  This final task will provide a good foundation for further considerations of the theoretical import of my work.}

\paragraph{Feature Extraction and Model Learning} The geometric features that my methodology extracts from contextually projected subspaces will be measurements of and between vectors: vector norms and angles, basically.  The calculation of these features will therefore employ standard linear algebraic techniques for computing cosines and lengths given sets of coordinates in high dimensional spaces.  I will compose and then normalise matrices of these measures to be passed to two different categories of models, corresponding to two different semantic tasks: linear regression models geared towards providing ranking of word-pairs to be compared to human judgements of semantic relationships construed along a continuous scale, and logistic regression models trained to match binary human judgements about the classification of word-pairs in terms of some semantic phenomenon.  In the first case, I'll apply a standard polynomial least squares linear regression, treating geometric features as independent variables and learning to predict a human rating by assigning weights to each feature in a geometric feature vector extrapolated from a subspace projected from an analysis of the corresponding input terms.  In the second case, I'll use an iterative regression technique to learn a set of weights to apply to features that will then be passed through a logistic function, in this case learning a threshold for determining the class of the input, again to be compared against human classification decisions of the same data.

\paragraph{Result Scoring and Hypothesis Testing} In line with the two different types of task described above -- rankings and classifications -- I will use two different measures to score the correlation between computational output and human judgements.  In the first instance, I will compare the values assigned to a set of word-pairs by a computational methodology to the values for the same word-pairs determined by human evaluators using Spearman's rank correlations, measuring the degree of monotonicity in the correlation between the two evaluations of the data.  In the second instance, I'll use f-scores, or the harmonic mean of precision and recall scores as compared to the human evaluation of the data.

Having derived these results, one evaluative technique will be to compare the results generated using my methodology to baseline results involving naive classification techniques in particular and also results returned by other models trained on the same underlying corpus.  To make these comparisons, I will apply quantitative techniques to test the null hypothesis that the difference in results can be explained in terms of random variation in model input.  In the case of Spearman's correlations of word pair ratings, the metric of choice will be the Fisher r-to-z transform, offering a stable comparison between pairs of data as a function of their correlation coefficients and the scale of the data itself.  In the case of figurative language classification and analogy completion, permutation tests will be used, taking the difference in score associated with two different sets of outputs and then testing, through random iterations, the probability that random split in either of the two outputs would generate a larger difference in scores.  \revAK{1}{For the purposes of the experiments performed in this thesis, I will consider differences in results with probability of chance observation of $p < .01$ to be statistically significant, and will compare between selective results with alignments in parameters chosen to be indicative of overall model tendencies.  See} \cite{RastogiEA2015} \revAK{1}{and} \cite{FaruquiEA2016} \revAK{q}{for insightful considerations of the application of statistical significance in natural language processing experiments.}

\paragraph{Quantitative and Qualitative Analysis} \revAK{2}{As outlined above, the methods for evaluating results for the various experiments performed in this thesis will be, first and foremost, quantitative.  I will also, however, selectively perfrom qualitative analysis on aspects of model output.  These selections will in general be made based on assessments of instances of model output that are exemplary of overall patterns: so, for instance, I will choose depictions of three-dimensional projections that are at either extent and very close to the centre of the distribution of model ratings for word similarity and relatedness, and likewise for metaphor and coercion.  To the extent that there is a systematic rationale for the selection of subject material for qualitative analysis, I will indicate this at appropriate points in the text.}

\section{The Layout of the Thesis}
The next chapter will, as is standard in a manuscript of this sort, delve deeper into the background supporting, inspiring, and, in some cases, compelling my own methodology.  Where I will diverge slightly from a standard computer science literature review is in my focus on theoretical background, but an understanding of the ideas about language and cognition motivating my work is essential to appreciating the actual technical mechanisms of the methodology that is subsequently developed, as well as my aspiration to develop further theoretical insight based on my results, engendering a virtuous cycle of theory and practice.  The theoretical background of my research will in any event be supplemented with a survey of relevant technical work in the areas of semantic modelling that I will target, and Chapter~\ref{sec:data} in particular will present an overview of technical work in the distributional semantic paradigm.  I will furthermore be introducing additional background related to the specific semantic tasks towards which I'll be directing my methodology, which will be outlined in Chapters~\ref{chap:relsim}, \ref{chap:figurative}, and \ref{chap:analogy}.

Chapter~\ref{chap:theory} will present the theoretical framework for my methodology, considering the way that conceptual schemes might arise from taking contextual perspectives on spatially construed representations.  Beginning with an overview of the linguistic suppositions that define the theoretical boundaries of my approach, this chapter will proceed with a consideration of the nature of lexical representations (Chapter~\ref{sec:lexsem}), the dynamic generation of conceptual perspectives on semantic spaces (Chapter~\ref{sec:sensitivity}), the construction of representations by as vectors of interpretable statistical features (Chapter~\ref{sec:litdims}), and the way that such representations facilitate semantic interpretation through geometric configurations (Chapter~\ref{sec:interpretable}).  This grounding will then be transformed into a description of a technical instantiation in Chapter~\ref{chap:method}.  The details covered here will include the generation, traversal, and statistical modelling of a large scale corpus (Chapter~\ref{sec:pmi}), a method for contextually projecting subspaces from a base model of co-occurrence statistics (Chapter~\ref{sec:project}), and a description of the geometric features I will use to measure semantic relationships in contextual subspcaes (Chapter~\ref{sec:geotext}).  Additionally, Chapter~\ref{sec:math} will offer a mathematical grounding for the productivity of geometric measures in spaces of literal co-occurrence dimensions, and Chapter~\ref{sec:poc} will provide a basic proof of concept by way of an experiment involving the recapitulation of a knowledge base designed specifically to test my methodology.

The second half of the thesis will present an empirical investigation of the model developed in Chapters~\ref{chap:theory} and \ref{chap:method}.  In particular Chapter~\ref{chap:relsim} will outline two experiments seeking to learn to predict human rating of \del{of} the connected but distinct semantic phenomena of word relatedness (Chapter~\ref{sec:relperiment}) and word similarity (Chapter~\ref{sec:simperiment}).  An assessment of the statistical geometry peculiar to each phenomenon will be offered in Chapter~\ref{sec:litpare}, leading to a consideration of how empirical results can be used to gain further theoretical insight into the nature of semantics.  Chapter~\ref{chap:figurative} will turn to the classification of figurative language, beginning with an experiment applying my methodology to labelling word pairs for metaphoricity in Chapter~\ref{sec:metperiment} followed by a similar experiment pertaining to semantic type coercion in Chapter~\ref{sec:coerperiment}.  This experimental work will once again lend itself to a reconsideration of theoretical assumptions, in this case assumptions about the informational nature of figurative language, in Chapter~\ref{sec:intercomp}.

Chapter~\ref{chap:analogy} will then describe a final experiment on analogy completion.  This experiment stands apart from the others to a certain extent, first in that it involves a task of a generative nature, requiring my methodology to output words rather than measures, and second in that it is an entirely unsupervised task, falling back on the geometry of contextualised lexical semantic subspaces alone.  Chapter~\ref{sec:parallel} will present the geometric approach to analogy, and then Chapter~\ref{sec:anatext} will explore ways in which contextually selected geometries facilitate analogical modelling.  Finally, Chapter~\ref{sec:datanote} will take a critique of the analogical dataset used and some of the assumptions inherent in its construction as an opportunity to raise some questions about the degree to which analogies can or cannot be understood in terms of mappings between well structured conceptual domains.  This will then bring questions of the relationship between semantics and geometry in contextually specified spaces of co-occurrence back into focus, setting the scene for a final consideration of the way that my context sensitive methodology permits a theoretical view of language as an element in the cognitive apparatus of environmental affordances, presenting in particular opportunities for meaning-making that challenge the boundaries of traditional computational approaches to conceptual modelling.

