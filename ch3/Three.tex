\chapter{A Model for Constructing Concepts on the Fly}
This chapter is concerned with a theoretical overview and a technical description of a novel distributional semantic model designed to map words into conceptually productive geometric relationships.  The model is grounded in the corpus based technique described at the end of the previous chapter, and so builds up representations in the form of word-vectors the features of which are derived from observations of word co-occurrences in textual data.  This model is predicated upon three interrelated desiderata:

\begin{enumerate}
\item The model should be dynamically sensitive to context;
\item The model should function in a way that is transparent and operationally interpretable;
\item The model should be situate words in spaces which are likewise geometrically interpretable.
\end{enumerate}

In the following three sections, each of these requirements will be analysed in the context of the underlying theoretical context.  This analysis is performed with the immediate project of designing a statistical model for mapping words to concepts in mind, and each element of the profile of desirable properties will be explored with this in mind.  Then finally, in a fourth section, the fundamental implementation of the model will be described in technical detail.

\section{Dynamic Context Sensitivity}
At the heart of the technical work described in this thesis is an insight which is broadly accepted by theoretical linguists and philosophers of language: word meaning is always contextually specified.  This wisdom is built into the foundations of both formal semantics \citep{Montague} and pragmatics \citep{Grice}, and is likewise taken into account in contemporary context-free approaches to syntax.  As evident from the implementations of conceptual models surveyed in the previous chapter, however, the computational approach has generally relied on the idea that concepts can, at some level of composition, be cast as essentially static representations.

One of the primary goals of the work presented in this thesis is to explore empirical methods for moving beyond this constraint.

With that said, the importance of context has certainly not been ignored by statistically oriented computer scientists.  Indeed, \cite{BaroniEA2014b} make a case for vector space approaches to ``disambiguation based on direct syntactic composition'' (p. 254), arguing that the linear algebraic procedures used to compose words into mathematically interpretable phrases and sentences in these types of models result in a systemic contextualisation of words in their pragmatic communicative context.  \cite{ErkEA2008} outlines an approach that models words as sets of vectors including prototypical lexical representations capturing information about co-occurrence statistics and ancillary vectors representing \emph{selectional preferences} \citep[\emph{a la}][]{Wilks1978} gleaned from an analysis of the syntactic roles each word plays in its composition with other words.  These composite vector sets are then combined in order to consider the proper interpretation of multi-word constructs of lexically loose or ambiguous nouns and verbs.  In subsequent work \citep{ErkEA2010}, the same authors describe a model which selects \emph{exemplar} word-vectors from, again, composites of vectors, in this case extracted from observations of specific compositional instances of the words being modelled.  In the first instance, composition is the mechanism by which word meaning is selectively derived, while in the second instance observations of composition are the basis for constructing sets of representational candidates to be selected situationally.

The model presented in this thesis is motivated by a premise similar to the one explored by \citeauthor{ErkEA2008}: there should be some sort of selectional mechanism for choosing the way that a word relates to other words in context.  I would like to push this agenda a even further, though.  In light of relevance theoretic investigations into the 

Following on \citepos{Barsalou} insight into the \emph{haphazard} way in which concepts emerge situationally, and likewise \citepos{Carston}

I propose that the mechanism for contextually mapping out conceptual relationships between representations of words should be as open ended as possible, ideally lending itself to the construction of novel conceptual relationships in the same way that the state space of possible word combinations offers an effectively infinite array of linguistic possibilities.

\section{Literal Dimensions of Co-Occurrence}
\cite{Baroni} has described the former as \emph{counting} and the latter as \emph{predicting}, but it must be noted that both methods are very much grounded in observations about the co-occurrence characteristics of vocabulary words across large bodies of text.

\section{Interpretable Geometry}
It is important at this point to distinguish between two different aspects of the model being outlined here, both of which are intended to fulfil a criterion of interpretability.  On the one hand, we have the selectional mechanism, the principles of which have been outlined in the preceding section.

\section{A Computational Model}

As is generally the case with data cleaning, these measures are prone to error

but one of the strengths of the subspace projection technique that my model uses is its resilience to noise.  So, for instance, misspellings will be categorised as highly anomalous co-occurrence dimensions and are therefore unlikely to be contextually selected (or, if they are regularly encountered enough to be contextually significant, there may well be 