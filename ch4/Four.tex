\chapter{Conceptual Clusterings: Similarity and Relatedness}
In this chapter, I present the first stage of empirical results using the methodology described in the previous chapter.  The tests described here are aimed at demonstrating my model's capacity for generating \emph{ad hoc} geometries in which words cluster in ways that broadly correspond to conceptual categories.  In each case, specific spaces are generated based on input in the form of two words to be compared to one another in terms of their relatedness to one another, with this degree of relatedness to be contrasted to the relatedness between a number of other terms in a test set.  As such, \emph{context} here is taken to be inherent in each word pairing, outside any broader sentential situation.  The spaces generated by my model at this stage are thus delineated by dimensions selected through an analysis of the word-vectors corresponding to each term in an input word pair, the basic intuition being that these spaces should be representative of the conceptual context suggested by the pairing itself.

These are not yet conceptual spaces in the sense of \cite{Gardenfors2004}: the only claim we can make about the actual dimensions of these spaces is that values along them correspond to statistics about word co-occurrences, and directionality in the spaces cannot be interpreted, aside from to say that either proximity to a central point or distance from the origin corresponds to a degree of inclusion in a conceptual category.  This is nonetheless taken to be a step towards the description of a properly conceptual space.  In the case of the word-ranking techniques described below in particular, we are describing \emph{de facto} convex spaces of words, defining the central point of these spaces in a way that maps to some extent to G\"{a}rdenfors's notion of prototypes, and with the additional idea, in the case of norm-based metrics, that movement away from the origin of a space indicates an increasing degree of conceptual relevance.  So these tests are offered as a first step towards developing a theory of the conceptual geometry of the spaces afforded by my model.

Two separate experiments are described, the first designed specifically as a proof of concept demonstrating the model's capacity for building conceptually viable spaces based on context-specific input, the second taking two broadly used NLP tasks as the basis for further exploration of the model's space projecting capacity.  Similar techniques for projecting context-specific subspaces are employed in each.  The first experiment, which has been designed specifically to test my model and emerged out of an exchange with reviewers in the course of preparing an article for publication, uses two straightforward techniques for defining words within a region of a space as conceptual constituents, while the second experiment expands upon these techniques with a direct geometric analysis.  Before describing each of these experiments in turn, however, I will devote a section to a brief outline of another set of distributional semantic models which will be used comparatively throughout all the experiments described in this thesis.

\section{A Neural Network for Learning Word Vectors}
For the experiments described here and throughout the rest of this thesis, the neural network based models outlined by \cite{MikolovEA2013b,MikolovEA2013c} under the rubric \texttt{word2vec} will be used as a point of comparison.  These models have received a remarkable degree of attention in the NLP literature since their introduction a few years ago, so much so that the software was mentioned by name in 116 out of the 230 long papers published in the 2016 Proceedings of the Meeting for the Association for Computational Linguistics \cite{ErkEA2016}.  The models have been taken as a source for representations of words \emph{embedded} in vector spaces trained on large scale textual data, applied to tasks ranging from 

The system is comprised of two different neural network architectures for generating word-vector representations based on traversals of large scale corpora.  The \emph{contextual bag of words} (CBOW) technique treats the terms in a co-occurrence window surrounding a target word $w$ as input and attempts to learn a representative word-vector $\overrightarrow{w}$ that is predicted by processing the input word-vectors through a recursive neural network.  The \emph{skip-gram} technique, on the other hand, treats the representation $\overrightarrow{w}$ itself as input to a network which learns to predict a sequence of word-vectors representing words on either side of the target word.  In both cases, the model updates the scalars of the target word vectors in order to move them closer to the vectors representing each co-occurrence in which they're observed.  In the case of the CBOW model, the terms co-occurring within a given window of the target word are combined into an average vector for the purpose of each training observation; with the skip-gram model, the selection of target output word-vectors is weighted based on their distance from the input word-vectors, and the model optimises the probability of two word vectors interpreted via the softmax function \citep[see][]{MikolovEA2013c} for more details}.

In addition to the size of the co-occurrence window, model parameters include the number of iterations of the corpus, the architecture of the single-layer network connecting input to output vectors, and, in the case of the skip-gram model, a rate of negative sampling by which random sets of words are taken as instances of non-co-occurrences and used to push the corresponding word-vectors away from the input word-vector.  The skip-gram model, with its sensitivity to word order, has been reported to perform particularly well on analogy completion task involving semantic similarity, so for instance in discovering the relationship \emph{king:queen::man:woman}.  The CBOW model, on the other hand, has performed better on what the authors have described as \emph{syntactic}\footnote{Following \cite{Sausaurre} and, in the context of computational linguistics, \cite{Sahlgren}, I would prefer the distinction \emph{paradigmatic} versus \emph{syntagmatic} in place of syntax versus semantics, though it should be noted that in a very clear sense all distributional semantic models are primarily concerned with both semantics and pardigms, using syntax and syntagm as the mechanism for discovering these semantic relationships.} analogies such as \emph{good:better::bad:worse}.

Here, the skip-gram and CBOW techniques of \texttt{word2vec} will be taken as exemplars of general-purpose distributional semantic modelling.  For the purposes of a fair comparison, I've trained instances of both models using the same cleaned corpus described in the previous chapter and used to train my own model.  The presumption, corroborated by the wide applications found for the models described by various authors over the past three years, is that this approach provides a general framework for generating a space in which word-vectors relate to one another in conceptually productive ways.  The primary difference between the vectors learned through non-linear regression by \texttt{word2vec} and the vectors representing word co-occurrence statistics derived by my model is that \texttt{word2vec} produces dense vectors whose dimensions cannot be individually interpreted as corresponding to any specific set of observations across a corpus, whereas my model generates a base space of sparse vectors for which each dimensions maintains its status as an indication about a tendency of co-occurrences with a specific term.  It is this interpretability and the corresponding dimensional selectability, that gives my model its power of contextualisation, which will now be explored empirically.

\section{A Proof of Concept}
In this section, I present the first experiment performed using my contextually dynamic distributional semantic model.  The gist of this experiment is to take a word pair representing a compound noun -- for instance, \emph{body part} -- and see if the model, which, as described in Chapter~\ref{sec:}, has a vocabulary consisting exclusively of single words, can use the word pair to contextual a space where other words conceptually related to that compound noun can be found in a systematic way.  This is conceived of as an entailment task, in that I will attempt to find phrases considered to be categorical constituents of the concept represented by the word pair, taking the WordNet lexical taxonomy as a ground truth.  There is a scholastic back story here.

The earliest results using my model were reported in \cite{AgresEA2015}, where this work was couched in a larger question about the evaluation of computational creativity.  A colleague was wondering if we could find a way to systematically come up with phrases that could be taken as objective labels for characteristics of various creative domains -- for instance, \textsc{musical creative}, \textsc{literary creativity}, and so forth -- that could then be used as prompts in querying human subjects about their responses to creative artefacts in the context of a psychological study.  While there was scope for extracting terms relevant to a conceptual domain in existing distributional semantic models, the problem of compound domain labels proved problematic: simply taking the midpoint between the two components of the label in, for instance, a \texttt{word2vec} model as an anchor and then exploring the nearby space yielded terms that were somewhat similar to both domains, but not particularly relevant to the overall concept.  We found improved results by instead defining a new space delineated only by dimensions deemed to be highly relevant to both terms involved in the compound phrase.

That first round of results, initially reported in a conference paper, evolved into a journal article \citep{McGregorEA2015c}.  That second article was originally conceived of as an extension of the psychologically motivated work presented in the original literature, but reviewers with a computer scientific bias were more interested in seeing further results targeting the kind of large-scale data-oriented tasks favoured by computational linguists rather than domain specific tests involving small groups of human participants offering responses in a controlled setting.  In response to this directive, we devised a task involving the recapitulation of the data contained in WordNet.  What follows is a description of an updated version of this task and a report of the latest stage of results on this task using my model.

\subsection{Taxonomical Data}
The WordNet database, described by its original architects as 

It organises its vocabulary into \emph{synsets} of word senses associated with a given lexical form, with each synset representing a word sense associated with a set of \emph{lemmas} embodying the range of words actually used to denote that word sense.

\begin{table}
\begin{tabular}[lrrrrrrrrr]
\hline
& \multicolumn{2}{c}{\textsc{joint}} & \multicolumn{2}{c}{\textsc{indy}} & \multicolumn{2}{c}{\textsc{zipped}} & \multicolumn{2}{c}{\textsc{w2v}} \\
& \multicolumn{1}{c}{high} & \multicolumn{1}{c}{full} & \multicolumn{1}{c}{high} & \multicolumn{1}{c}{full} & \multicolumn{1}{c}{high} & \multicolumn{1}{c}{full} & \multicolumn{1}{c}{sg} & \multicolumn{1}{c}{cbow} \\
\hline
psychological feature 
\end{tabular}
\end{tabular}
\end{table}

\section{Word Similarity in Context}

For the purpose of this analysis, we'll examine both ranking metrics and metrics extracted directly from the geometric features of the space projected for each candidate word pair.

\begin{table}
\begin{tabular}{lrrrr|rrrr}
\hline
& \multicolumn{4}{c|}{wordsim353} & \multicolumn{4}{c}{simlex999} \\
& \multicolumn{2}{c}{2x2} & \multicolumn{2}{c|}{5x5} & \multicolumn{2}{c}{2x2} & \multicolumn{2}{c}{5x5} \\
& \multicolumn{1}{c}{20} & \multicolumn{1}{c}{200} & \multicolumn{1}{c}{20} & \multicolumn{1}{c|}{200} & \multicolumn{1}{c}{20} & \multicolumn{1}{c}{200} & \multicolumn{1}{c}{20} & \multicolumn{1}{c}{200} \\
\hline
\textsc{Joint} & 0.372 & 0.387 & 0.403 & 0.393 & 0.138 & 0.179 & 0.128 & 0.155 \\
\textsc{Indy} & 0.371 & 0.400 & 0.410 & 0.440 & 0.134 & 0.173 & 0.111 & 0.149 \\
\textsc{Zipped} & 0.326 & 0.386 & 0.347 & 0.384 & 0.140 & 0.165 & 0.147 & 0.147 \\
\textsc{Merged} & 0.386 & 0.456 & 0.437 & 0.485 & 0.160 & 0.191 & 0.138 & 0.163 \\
\hline
\end{tabular}
\end{table}

\begin{table}
\begin{tabular}{lrrrrrrrrrr}
\hline
\textsc{space} & dis & v-cos & c-cos & n-cos & $n_1,n_2$ & $m_1,m_2$ & $c_1,c_2$ & c-dis & m-rat & n-rat \\
\hline
\textsc{Joint} & -0.353 & 0.484 & 0.594 & 0.283 & 0.597 & 0.485 & -0.486 & 0.227 & 0.205 & 0.274 \\
\textsc{Indy} & -0.249 & 0.597 & 0.607 & 0.252 & 0.341 & 0.170 & -0.583 & 0.131 & 0.084 & 0.115 \\
\textsc{Zipped} & -0.247 & 0.511 & 0.561 & 0.262 & 0.563 & 0.444 & -0.493 & 0.306 & 0.089 & 0.129 \\
\textsc{Merged} & -0.250 & 0.413 & 0.550 & -0.065 & 0.565 & 0.150 & -0.415 & 0.114 & -0.169 & 0.613 \\
\hline
\end{tabular}
\caption{Wordsim353, 20 dimensions, 2x2 co-occurrence window.}
\end{table}

\begin{table}
\begin{tabular}{lrrrrrrrrrr}
\hline
\textsc{space} & dis & v-cos & c-cos & n-cos & $n_1,n_2$ & $m_1,m_2$ & $c_1,c_2$ & c-dis & m-rat & n-rat \\
\hline
\textsc{Joint} & -0.203 & 0.480 & 0.603 & 0.399 & 0.626 & 0.453 & -0.449 & 0.231 & -0.041 & 0.121 \\
\textsc{Indy} & -0.213 & 0.598 & 0.667 & 0.495 & 0.373 & 0.135 & -0.571 & 0.103 & 0.032 & 0.089 \\
\textsc{Zipped} & -0.181 & 0.464 & 0.592 & 0.442 & 0.628 & 0.436 & -0.423 & 0.297 & -0.041 & 0.056 \\
\textsc{Merged} & -0.180 & 0.435 & 0.603 & 0.140 & 0.599 & 0.086 & -0.417 & 0.067 & -0.318 & 0.643 \\
\hline
\end{tabular}
\caption{Wordsim353, 200 dimensions, 2x2 co-occurrence window.}
\end{table}

\begin{table}
\begin{tabular}{lrrrrrrrrrr}
\hline
\textsc{space} & dis & v-cos & c-cos & n-cos & $n_1,n_2$ & $m_1,m_2$ & $c_1,c_2$ & c-dis & m-rat & n-rat \\
\hline
\textsc{Joint} & -0.467 & 0.558 & 0.644 & 0.383 & 0.577 & 0.461 & -0.560 & 0.185 & 0.200 & 0.284 \\
\textsc{Indy} & -0.300 & 0.644 & 0.639 & 0.287 & 0.329 & 0.159 & -0.627 & 0.086 & 0.045 & 0.066 \\
\textsc{Zipped} & -0.393 & 0.589 & 0.593 & 0.346 & 0.498 & 0.371 & -0.570 & 0.307 & 0.027 & 0.059 \\
\textsc{Merged} & -0.290 & 0.431 & 0.608 & -0.050 & 0.547 & 0.123 & -0.458 & 0.056 & -0.184 & 0.664 \\
\hline
\end{tabular}
\caption{Wordsim353, 20 dimensions, 5x5 co-occurrence window.}
\end{table}

\begin{table}
\begin{tabular}{lrrrrrrrrrr}
\hline
\textsc{space} & dis & v-cos & c-cos & n-cos & $n_1,n_2$ & $m_1,m_2$ & $c_1,c_2$ & c-dis & m-rat & n-rat \\
\hline
\textsc{Joint} & -0.343 & 0.574 & 0.651 & 0.525 & 0.599 & 0.447 & -0.552 & 0.239 & -0.029 & 0.183 \\
\textsc{Indy} & -0.241 & 0.650 & 0.696 & 0.555 & 0.366 & 0.121 & -0.626 & 0.069 & -0.003 & 0.074 \\
\textsc{Zipped} & -0.290 & 0.557 & 0.628 & 0.587 & 0.588 & 0.430 & -0.520 & 0.334 & -0.067 & 0.050 \\
\textsc{Merged} &-0.208 & 0.445 & 0.675 & 0.071 & 0.583 & 0.083 & -0.464 & 0.037 & -0.325 & 0.692 \\
\hline
\end{tabular}
\caption{Wordsim353, 200 dimensions, 5x5 co-occurrence window.}
\end{table}

\begin{table}
\begin{tabular}{lrrrrrrrrrr}
\hline
\textsc{space} & dis & v-cos & c-cos & n-cos & $n_1,n_2$ & $m_1,m_2$ & $c_1,c_2$ & c-dis & m-rat & n-rat \\
\hline
\textsc{Joint} & -0.216 & 0.284 & 0.334 & 0.159 & 0.299 & 0.255 & -0.296 & 0.078 & 0.115 & 0.128 \\
\textsc{Indy} & -0.204 & 0.351 & 0.359 & 0.176 & 0.107 & 0.006 & -0.343 & -0.027 & 0.070 & 0.088 \\
\textsc{Zipped} & -0.263 & 0.333 & 0.348 & 0.201 & 0.259 & 0.159 & -0.331 & 0.087 & 0.078 & 0.111 \\
\textsc{Merged} & -0.206 & 0.267 & 0.350 & -0.020 & 0.274 & -0.002 & -0.261 & -0.025 & -0.035 & 0.376 \\
\hline
\end{tabular}
\caption{Simlex999, 20 dimensions, 2x2 co-occurrence window.}
\end{table}

\begin{table}
\begin{tabular}{lrrrrrrrrrr}
\hline
\textsc{space} & dis & v-cos & c-cos & n-cos & $n_1,n_2$ & $m_1,m_2$ & $c_1,c_2$ & c-dis & m-rat & n-rat \\
\hline
\textsc{Joint} & -0.213 & 0.331 & 0.368 & 0.291 & 0.308 & 0.215 & -0.317 & 0.019 & -0.015 & 0.064 \\
\textsc{Indy} & -0.179 & 0.357 & 0.388 & 0.318 & 0.117 & -0.009 & -0.335 & -0.044 & 0.051 & 0.117 \\
\textsc{Zipped} & -0.214 & 0.312 & 0.357 & 0.304 & 0.292 & 0.138 & -0.296 & 0.048 & -0.025 & 0.102 \\
\textsc{Merged} & -0.144 & 0.247 & 0.363 & 0.079 & 0.278 & -0.013 & -0.234 & -0.082 & -0.061 & 0.364 \\
\hline
\end{tabular}
\caption{Simlex999, 200 dimensions, 2x2 co-occurrence window.}
\end{table}

\begin{table}
\begin{tabular}{lrrrrrrrrrr}
\hline
\textsc{space} & dis & v-cos & c-cos & n-cos & $n_1,n_2$ & $m_1,m_2$ & $c_1,c_2$ & c-dis & m-rat & n-rat \\
\hline
\textsc{Joint} & -0.243 & 0.295 & 0.325 & 0.185 & 0.229 & 0.176 & -0.314 & 0.058 & 0.093 & 0.122 \\
\textsc{Indy} & -0.168 & 0.311 & 0.307 & 0.158 & 0.083 & 0.004 & -0.301 & -0.033 & 0.053 & 0.082 \\
\textsc{Zipped} & -0.296 & 0.338 & 0.326 & 0.201 & 0.180 & 0.080 & -0.338 & 0.108 & 0.054 & 0.108 \\
\textsc{Merged} & -0.182 & 0.208 & 0.343 & -0.063 & 0.218 & -0.014 & -0.228 & -0.030 & -0.064 & 0.350 \\
\hline
\end{tabular}
\caption{Simlex999, 20 dimensions, 5x5 co-occurrence window.}
\end{table}

\begin{table}
\begin{tabular}{lrrrrrrrrrr}
\hline
\textsc{space} & dis & v-cos & c-cos & n-cos & $n_1,n_2$ & $m_1,m_2$ & $c_1,c_2$ & c-dis & m-rat & n-rat \\
\hline
\textsc{Joint} & -0.252 & 0.324 & 0.339 & 0.314 & 0.228 & 0.139 & -0.307 & 0.025 & 0.030 & 0.085 \\
\textsc{Indy} & -0.145 & 0.307 & 0.342 & 0.307 & 0.087 & -0.005 & -0.282 & -0.053 & 0.068 & 0.104 \\
\textsc{Zipped} & -0.259 & 0.317 & 0.330 & 0.337 & 0.204 & 0.065 & -0.298 & 0.061 & 0.030 & 0.112 \\
\textsc{Merged} & -0.117 & 0.170 & 0.332 & 0.050 & 0.212 & -0.008 & -0.167 & -0.078 & -0.015 & 0.329 \\
\hline
\end{tabular}
\caption{Simlex999, 200 dimensions, 5x5 co-occurrence window.}
\end{table}

So, for instance, the ostensibly conceptually compatible pair (\emph{reality,fantasy}) scores quite low at 1.03, while (\emph{door,gate}) are considered somewhat similar at 5.25, and (\emph{business,company}) are rated, notwithstanding the considerable ambiguity of either lexical form outside of the context of one another, at 9.02.

Here again \citepost{Sausaurre} dichotomy of paradigm and syntagm once again reveals itself: we might easily argue that \citepost{Hill} data is focusing on the relatively syntagmatic relationship between words like \emph{business} and \emph{company} which might easily co-habitate in a single sentence, where the WordSim data incorporates both these types of pairings and the more paradigmatic relationship between \emph{reality} and \emph{fantasy}, which would only be mentioned together as a point of antonymy.  This observation begs a question not generally addressed by the tasks generally explored by NLP researchers: what to make of more strictly syntagmatic relationships such as that between \emph{shiny} and \emph{new}?  Here we might easily imagine how, \emph{in the correct conceptual context and the corresponding profile of co-occurrences}, relationship spanning the boundaries of semantic categories should be easily discovered by my dynamically context sensitive distributional semantic model.
