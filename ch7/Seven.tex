\chapter{Conclusion}
Looking back on the work presented here, \emph{perspective} seems like a good word to apply to various aspects of my research.  First of all, the methodology that I have developed is rooted in a theoretical perspective, or maybe more a perspective on a theoretical domain within the study of language and cognition.  I prefer to call this a perspective rather than, for instance, a \emph{stance} because I have at least attempted to treat the theoretical background surveyed in Chapter~\ref{chap:background} and applied abstractly to the notion of semantic models in Chapter~\ref{chap:theory} as a facilitator rather than as a constraint on the empirical work that follows.  Then, in the approach to lexical semantic modelling presented as an idea in Chapter~\ref{chap:theory} and developed as a computational implementation in Chapter~\ref{chap:method}, I have put forward a methodology rooted in the notion that there is much to be gained from considering semantic information in terms of contextual perspectives taken on data.

In the experiments on the quantitative modelling of semantic phenomena described in Chapters~\ref{chap:relsim} and \ref{chap:figurative}, the geometric interpretation of the statistical features used to learn models for rating or identifying these phenomena provides traction for probabilistic analysis according to the mathematical provisions of the methodology, and, in turn, speculation about the cognitive correlates of frequentist observations about language use.  One of the stipulations of my technical approach has been that the operations of a quantitative model should continuously submit themselves to interpretative analysis, offering once again a perspective on the way that observable and indeed countable characteristics of word use build up into an emergent model of semantic relationships.  By maintaining a connection between the features submitted to machine learning processes and the underlying observations of large scale data, I have sought to demonstrate how we can treat the supervised learning of models of semantic phenomena as not only a task-completing mechanism, but also an enabler of subsequent theoretical insight into the nature of language use.

The experiments on analogy modelling and completion in Chapter~\ref{chap:analogy} offer an opportunity to exploring my methodology's capacity for generating semantically productive geometric lexical representations in an unsupervised manner.  The categorical analysis of a couple of my techniques for subspace projection on analogy completion tasks provides insight into the way that data-driven semantic space type models both succeed and fail at delineating the sub-conceptual regionalism crucial to a productive semantic space.  This type of forthright analysis, continually seeking to return to the data itself and the way that the dynamics of representations are tied to the computation of observed language use, provides the basis for a more qualitative assessment of the functioning of a model of intricately interactive units, often leading to challenges directed at the data used to evaluate a methodology in the first place.  Ultimately, my geometric approach opens itself up to the potential for a radical approach to semantic modelling, not taking for granted the supervenience of the lexicon upon a well defined framework of composite conceptual representations; instead, with a context sensitive approach to distributional semantics, we can afford to imagine lexical-conceptual relationships as abstract but nonetheless graspable structures perpetually emerging from the dynamic between an agent and an environment, and the lexicon itself as a facility wrapped up with conceptualisation rather than a symptom of it.

The question which motivated this research was whether data about word co-occurrences could, through the application of statistical analysis and by way of geometrically situated lexical representations, be translated into semantically useful information.  My hypothesis, that a routine of contextualisation taking the form of online projections -- perspectives -- from the statistical space, has been supported, inasmuch as results ranging from competitive to outstanding have been returned on a variety of tasks.  Moreover, the nature of my methodology has provided the basis for post-experimental analysis of the way that its representations are extrapolated from large scale data, and I consider this to be an achievement of at least as much value as the task-completing capacity of the methodology itself.  This thesis has been laced throughout with theoretical conjecture, and it is the interpretability of the operation of my techniques which permits a continual return to the philosophical assumptions and implications of my approach.  In these final few pages, I will seek to summarise the empirical results obtained over the last three chapters, consider how these overall results might point the way towards productive future work, and then finally reflect on the philosophical ramifications of my research.

\section{Summary}
Overall, my methodology has proved to be often competitive and occasionally outstanding in its performance on a handful of tasks covering a range of semantic phenomena.  The results on ranking relatedness and similarity indicate an approach that is at least in line with other recent work in the field, some of which imposes considerably more in the way of heuristics and pre-formulated information about the conceptual referents of words than my technique does.  While the results on similarity are, like with other distributional semantic models applied to the SimLex dataset, substantially worse than than the relatedness results, the comparison between the feature weights learned by linear models for addressing each phenomenon reveals interesting differences between the way that semantics play out in terms of statistical geometry, painting a picture of the way that a single analytic process might, in contextualised subspaces, provide a basis for the analysis of multiple axes of conceptual relationships between input terms.  In the end one of the most interesting outcomes of the experiments analysed in Chapter~\ref{chap:relsim} is the idea that there could be relatively blunt statistical features such as word frequency or co-occurrence dimension variance that present semantic signals.  In this respect my model could prove to be a useful tool for discovering quantitative features of language use which yield productive theoretical insights.

The results on metaphor classification are exceptionally strong, suggesting that contextualisation plays a significant role in identifying the degree to which a potentially compositional relationship between words can be analysed in terms of a transfer of information between conceptual domains.  Experiments with learning a model for graded ratings of metaphor from data annotated with merely binary tags suggest that a geometric approach aptly frames metaphor as a spectrum rather than as a well-defined phenomenon, and it is particularly interesting to note that the movement from literal to metaphoric language is marked by what appear to be stages of statistical shifts rather than a single smooth progression of geometric features.  Results on coercion classification are not as strong as the metaphor results; while this may to a certain extent be a product of the data itself, the balance of which makes coercion identification more difficult, the accuracy scores for the best performing models and a comparison with baselines and previous work on this dataset also paint a picture of a phenomenon that proves to be more elusive for my methodology.  Furthermore, the geometric analysis of coercion offer less in terms of a clear-cut analysis of the co-occurrence tendencies that characterise type shifting.

There is, of course, a close, and perhaps at times even ambiguous, relationship between coercion and metaphor: the interpretation of each phenomenon requires some allowance for lexical looseness combined with a step of contextualisation to refocus the a word that has been lured out of what might be seen as its native lexical habitat.  The difference is perhaps a matter of the nature of the underlying conceptual schemes implicated specifically in the interpretation of each of these distinctly non-literal phenomena, with the type shifting of coercion lending itself more readily to a hierarchical scheme of conceptual classes.  One way of putting this would be that, where the interpretation of metaphor seems to involve determining a context in which the conceptual deracination of a word's denotation can be understood in terms of an intensional transfer between domains, coercion interpretation comes across as more of a determination of the level of abstraction on which to consider the relationship between, for instance, a predicate and an argument.  Among other things, data involving scoring rather than just classifying potential instances of coercion would offer insight not only the operation of my methodology but also the way that humans cognitively approach this phenomenon.

Results on analogy completion are noteworthy, if not quite on par with the remarkable outcomes of the \texttt{word2vec} techniques, for which the data I use was originally designed.  It is in the case of the work on analogy that my methodology comes closest to empirically returning to its G\"{a}rdenforsian theoretical roots, with subspaces characterised by conceptual regions and directions representing general semantic correspondences.  Analogy stands out as something of a special case in the research presented here, in that, among other things, the geometric mechanism targeting analogy completion is entirely unsupervised: the hypothesis tested is that analogies should simply play out as predictable geometric configurations in an appropriately contextualised subspace.  The geometry specified, in line with other models, is that of the parallelogram, suggesting a conceptual coordination in terms of orientation and distance in a semantic space.  An analysis of optimal subspaces, though, indicates that there might be even more specific types of polygons implicated in the mapping of analogy: rectangles or squares with a certain orientation in relation to the edges of a positively valued subspace, for instance, might be even more strongly associated with the intensional coordination at play in analogy.  Choosing dimensions that are collectively best for facilitating these properties poses a potentially hard computational problem, however, as the angles and balances of side lengths of polygons are products of the overall situation of the shapes' vertices and cannot be even approximated based on a dimension-by-dimension analysis.

\section{Outlook}
The work described in this thesis has involved the extrapolation of contextual semantic models from digitised textual data through the application of information processing procedures interacting at many different stages along the passage from raw data to semantic information.  This methodology can be understood in terms of the parameters which correspond to these stages, which can be summarised as follows:

\begin{itemize}
\item[Data] The corpus selected for building a base space of co-occurrence statistics and the data cleaning procedures applied to this corpus;

\item[Word Counting] The method used for tabulating co-occurrence counts, which in the work described here has involved simply seeing how close words are to one another in a sentence but could also be calculated in terms of, for instance, distance along the branches of a dependency tree;

\item[Statistical Processing] The function applied to word frequencies and co-occurrence counts in order to derive the elements of a base space for subsequent contextual projection;

\item[Subspace Selection] The techniques, including the choice of contextualising input, for applying a statistical analysis of word-vectors to the selection of a set of dimensions for subsequent geometric semantic analysis;

\item[Geometry] The measures and mappings used to extrapolate semantic information from the situation of word-vectors in a contextualised subspace.
\end{itemize}

\noindent The research presented here has offered a sampling of possible combinations of these parameters; my intent has been to focus on higher level issues of theoretical grounding and philosophical import, in the context of a rigorous but also broad survey of my methodology's application to a handful of established natural language processing tasks.  This means that, to the extent that the results returned on various datasets are good enough to motivate future research with context sensitive distributional semantic models, there is already a considerable amount of work that can be done simply by way of exploring the parameter space of the methodology as it stands.  So for instance the type of language found in Wikipedia, which has particular, arguably even peculiar characteristics in terms of vocabulary, idiomaticity (or lack thereof), sentence length, and so forth may not be best suited for building dimensions that are amenable to contextualisation.  Likewise there might be more effective techniques for subspace selection, involving for instance an analysis of the relationship between word-vectors along a dimension, or even just an analysis of general statistical characteristic of dimensions such as mean and variance.

A particularly interesting topic for future research is the question of how best to translate word-counts into informative statistics about co-occurrence relationships.  The application of an information theoretical metric is attested in the literature and has proven fairly effective for the tasks described here, affording a mechanism for translating probabilities into geometric features, but there are a variety of other functions that can be applied to word-counts, as well.  Should logarithmic dimensions, for instance, be weighted by the probability of a joint occurrence of their correlates, allowing for a sort of entropic analysis of a dimension's distribution?  And does the warping factor of the smoothing constant and the shifting of ratios of probabilities prior to taking logarithms perhaps have an adverse effect on conceptual geometry?  The removal of these constants, which were introduced to enable the dimension selection process, may allow word-vectors to fall into an even more semantically interpretable geometry.  And if dimensions are constructed as probability distributions, then this opens them up to a potentially productive analysis in terms of probabilistic moments, providing another set of features that can possibly be mapped to the semantic character of subspaces.  There is even scope for applying a kind of reverse subspace selection procedure, seeking to induce, given a subspace which we know to be in some sense semantically productive, the kind of word-vectors which would indicate such a subspace.

In terms of analysing the geometry of subspaces, it may be worthwhile to consider any of these phenomena in terms of proximity by way of relative ranking within subspaces, rather than in terms of absolute geometric measures.  So, for instance, the similarity between pairs of words may be more accurately mapped as a function of the number of other word-vectors closer to either input word-vector, within a contextualised subspace, than either word is to the other: this could go some way towards addressing the issues of framing raised in Chapter~\ref{sec:frames}.  A similar measure could be applied to words' relative distance, in terms of rank rather than norm, from the origin.  And likewise, ranking measures could be applied to generic points of the space; in fact, this relativistic approach might serve to mitigate the influence of outliers in terms of maximum and mean vectors, lessening the impact of values that are simply aberrantly far from anything else by virtue of low independent frequencies combined with high joint frequencies.

More ambitiously, we might consider other ways of modelling the lattice of contextual subspaces itself.  The projections afforded by one of the base spaces that my methodology generates can, as outlined in Chapter~\ref{sec:litdims}, be understood as a power set of contexts, and so a discreet structure that can be thought of as an unweighted network of possible subspaces.  If we consider the relationships between dimensions, however, and in particular establish a metric of distance between dimensions, then the immense space of possible projections becomes a smooth manifold, a continuous space of at least suppositional positions within a lattice of unbounded subspaces.  This construct would have powerful mathematical attributes, potentially allowing for the modelling of semantic relationships in terms of differentiable movements across contexts rather than just as static relationships within dynamically selected but discreet subspaces, and opening my approach up to coordination with the promising techniques of the relatively new field of information geometry.

Finally, the nature of the tasks towards which my methodology can be applied, and the corresponding evaluation of performance on these tasks, deserves further consideration.  The type of datasets used here, which are fairly typical of the approach in natural language processing in general, seek to facilitate a quantitative evaluation of computational models.  This is well motivated, and the data used for exploring relatedness \citep{FinkelsteinEA2002} and similarity \citep{HillEA2015} are characterised by thoughtful consideration of some of the clinical, psychological background by way of word association studies, while the coercion data \citep{PustejovskyEA2010} is thoroughly grounded in the theoretical linguistic literature and, in its provision of sentential context, can be used a source to explore further degrees of contextualisation.  The task-oriented approach to evaluative computational linguistic systems inherent in these datasets has proved a valuable way of feeling out the capabilities and limitations of my methodology, and comparison to other approaches that have been applied to the same data likewise serves as a useful way of understanding where a contextual paradigm works and where it might be improved.

Nonetheless, any dataset configured in terms of decontextualised samples of language and scores is going to have biases built into it, from the way in which representative language instances are chosen to the decisions about what constitutes a correct or adequate output.  I maintain that a more qualitative approach to computational linguistics, orientated towards processing heterogeneous inputs and then subjected to a subsequent analysis in terms of the impact of outcomes rather than simply the scores of outputs will provide the basis for a much more thorough understanding of what the type of model I've described in this thesis might be able to do.  With this in mind, I propose that the digital humanities are an appropriate target domain for the development of my methodology.  A particular appeal of the humanities is the inherent appreciation of critical perspectives on data: rather than framing semantic modelling in terms of correct solutions to tasks and corresponding scores, there is a notion that any semantic commitment is wrapped up in a perspective on the world.  The connection between this philosophical stance and my methodology should be now be obvious: we can begin to imagine how, rather than providing a singular interpretation of linguistic data, a contextual approach to distributional semantics might offer a proliferation of different approaches that come equipped, by way of the interpretability of literal spaces of co-occurrence dimensions, with a mechanism for extrapolating not just an \emph{explanandum} but also an \emph{explanans} of model output.

There is a movement afoot to take the digital humanities beyond its foundation in textual studies, towards a more interpretive approach to handling on a massive scale the resources with which humanists have traditionally worked \citep[see][for an interesting discussion]{AllingtonEA2016}.  \cite{Moretti2013} has offered one version of a general framework for such an undertaking, and more concretely empirical work has already been initiated by \cite{HopeEA2010} and \cite{Ahnert2015}, with \citeauthor{HopeEA2010} in particular proposing a mechanism for interpreting a large scale digitised canon explicitly using the techniques of distributional semantics.  In addition to providing a tool for computationally establishing critical semantic perspectives, I propose that the interpretable nature of my methodology can provide further value by tying the output of a computational process directly to passages in the underlying data.  So, for instance, when it turns out that a particular set of dimensions is best suited for mapping out an analogical interpretation of language use in a work of literature, a contextual model would be able to refer a scholar back to the actual instances of co-occurrences between the words being targeted and the dimensions specified, offering an opportunity to discover unexpected concordances across a potential vast corpus.

\section{Direct Encounters with Meaning}
At the beginning of Chapter~\ref{chap:analogy}, I described analogy as a kind of semantic meta-phenomenon, offering a basis for the analysis of constituent words in terms of a variety of different forms of meaningfulness.  This is because, given an analogical alignment between two pairs of words, there is presumably some aspect of conceptual correspondence along both of the axes of parallelism.  Actually knowing the nature of this correspondence, however -- actually having the meaning, the intention -- is not necessarily a component of the analogical construct, and in the end there is arguably a sort of Chinese Room \citep{Searle1980} quality to having a well-formed but perhaps poorly understood analogy.  This gets back to a question raised at the very beginning of Chapter~\ref{chap:intro}: what can we really know about words from the outside, so to speak, just in terms of how they interact with the world that they are in?  If we have just one piece of information, that $A:B::C:D$, we know nothing about what any of those components denote, and almost nothing about \emph{how} they denote by way of their semiotic contingencies and constraints, their structure on whatever level of abstraction.  But if we can patch together enough instances of such analogies, cobbling one analogy to the next by matching one edge to another, can we eventually cash out the looming construct of hinged inferences to get something like meaningfulness?  In fact, in the end, do we really ever have anything else to work with in our pursuit of meaning?

In the end, it may actually be better to think of analogy and other attendant semantic phenomena as things that can be modelled without committing to any particular interpretation.  Such an approach seems truer to the pragmatic commitment to extemporaneous lexical specification: lexemes present themselves, just like any other object, however concrete or abstract, in the \emph{umwelt} of a cognitive agent, as perceptual opportunities for action, in the specific but perhaps not special case of language as an opportunity for meaning making.  So where does a distributional semantic approach get us?  Word frequency is not a thing that is directly perceived: though I could make an informed guess, I do not actually have a sense of how often I've encountered the word \emph{the} recently, versus the word \emph{thundercloud}, versus the word \emph{fulgurate}, and even if I did have this information it would seem far removed from the way those words each inform and evoke.  Meaning, on the other hand, is directly perceived: even on the abstract level of non-iconic, highly symbolic language, there is `the familiar physiognomy of a word, the feeling that it has taken up its meaning into itself,'' \citep[][p. 218]{Wittgenstein1953}.

What I finally claim that my contextual, geometric, interpretable take on distributional semantics points towards is pushing language into the domain of the perceivable, divesting it of the livery of its sometimes exceptional status as the intransigent medium of philosophy, shorting the circuit of language about concepts, concepts of language, and, eventually, the recursion of language about how language is about things.  Instead, in the statistically grounded geometry of a fleeting contextual semantic space, we have a chance to simply grasp at a structure with length and distance, making meaning in the same way that we might pick up a shoe to smash a fly or flap a newspaper to fan away smoke, just by applying what our situation in our environment affords us.  Under this regime, things like metaphors arise not as a consequence of computations about information transfer or even necessarily resolution of choices about optimal informational import, but simply as chance encounters with the possibilities of words as implements of conceptualisation and communication, conveying as much about how we mean as what we mean.  There are, of course, issues of supervenience and cognitive plausibility here, and I would be remiss to try to suggest that there is any reason to suspect isomorphism between my methodology and, for instance, neurological structures or psychological constructs.  All the same, on a certain level of abstraction, my approach provides access to meaning as something that is in the world in the form of a construct in an abstract space, that arises through contact with the ongoing situation in an environment, and that can be mapped back to a situation in a resource that is real and graspable.

What I am ultimately seeking to set up here is the groundwork for the application of computational techniques to phenomenologically oriented cognitive models.  This is why I've attempted to make my methodology conversant with, for instance, \citepos{Davidson1978} theory of metaphor, which is in the end about the way that language gets outside of the portage of propositions about situations in the world and into the actual fabric of the experience of existing.  If the phenomenal experience of reality -- the quality of perceiving and knowing in a world of objects and processes -- is, as \cite{Clark2016} puts it, \emph{controlled hallucination}, then words too are a layer of the veridical delusion, playing their part in the tumult of uncountable instances of tiny resolutions of uncertainty that ineffably works its way into the peculiar intentionality of being in the world.  In this work, I have sought to take words seriously as material evidence in the hunt for some sort of insight into the conditions under which meaning comes about, and I hope that my methodology, with its sensitivity to theoretical considerations and its mechanism for potentially unlimited semantic productivity, can serve as another implement in the resolution of these very hard questions.

