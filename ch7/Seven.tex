\chapter{Conclusion}
Looking back on the work presented here, \emph{perspective} seems like a good word to apply to various aspects of my research.  First of all, the methodology that I have developed is rooted in a theoretical perspective, or maybe more a perspective on a theoretical domain within the study of language and cognition.  I prefer to call this a perspective rather than, for instance, a \emph{stance} because I have at least attempted to treat the theoretical background surveyed in Chapter~\ref{chap:background} and applied abstractly to the notion of semantic models in Chapter~\ref{chap:theory} as a facilitator rather than as a constraint on the empirical work that follows.

Then, in the approach to lexical semantic modelling presented as an idea in Chapter~\ref{chap:theory} and developed as a computational implementation in Chapter~\ref{chap:method}, I have put forward a methodology rooted in the notion that there is much to be gained from considering semantic information in terms of contextual perspectives taken on data.  In the experiments on the quantitative modelling of semantic phenomena described in Chapters~\ref{chap:relsim} and \ref{chap:figurative}, there is an additional aspect of perspective at play, in that a perspective can be mapped to an interpretation, and one of the 

The geometry of my subspaces has provided the grounding for further analysis of the statistical characteristics of semantic phenomena thanks to the interpretable nature of the dimensions, and this in turn affords a reconnection of the data to ideas about cognition and language.

In these final few pages, I will seek to summarise the empirical results obtained over the last three chapters, consider how these overall results might point the way towards productive future work, and then finally reflect on the philosophical ramifications of my research.  This thesis has been laced throughout with theoretical 

\section{Summary and Outlook}
Overall, my methodology has proved to be often competitive and occasionally outstanding in its performance on a handful of tasks covering a range of semantic phenomena.  The results on ranking relatedness and similarity indicate an approach that is at least in line with other recent work in the field, some of which imposes considerably more in the way of heuristics and pre-formulated information about the conceptual referents of words than my technique does.  While the results on similarity are, like with other distributional semantic models applied to the SimLex dataset, substantially worse than than the relatedness results, the comparison between the feature weights learned by linear models for addressing each phenomenon reveals interesting differences between the way that semantics play out in terms of statistical geometry, painting a picture of the way that a single analytic process might, in contextualised subspaces, provide a basis for the analysis of multiple axes of conceptual relationships between input terms.  In the end one of the most interesting outcomes of the experiments analysed in Chapter~\ref{chap:literal} is the idea that there could be relatively blunt statistical features such as word frequency or co-occurrence dimension variance that present semantic signals.  In this respect my model could prove to be a useful tool for discovering quantitative features of language use which yield productive theoretical insights.

The results on metaphor classification are exceptionally strong, suggesting that contextualisation plays a significant role in identifying the degree to which a potentially compositional relationship between words can be analysed in terms of a transfer of information between conceptual domains.  Experiments with learning a model for graded ratings of metaphor from data annotated with merely binary tags suggest that a geometric approach aptly frames metaphor as a spectrum rather than as a well-defined phenomenon, and it is particularly interesting to note that the movement from literal to metaphoric language is marked by what appear to be stages of statistical shifts rather than a single smooth progression of geometric features.  Results on coercion classification are not as strong as the metaphor results; while this may to a certain extent be a product of the data itself, the balance of which makes coercion identification more difficult, the accuracy scores for the best performing models and a comparison with baselines and previous work on this dataset also paint a picture of a phenomenon that proves to be more elusive for my methodology.  Furthermore, the geometric analysis of coercion offer less in terms of a clear-cut analysis of the co-occurrence tendencies that characterise type shifting.

There is, of course, a close, and perhaps at times even ambiguous, relationship between coercion and metaphor: the interpretation of each phenomenon requires some allowance for lexical looseness combined with a step of contextualisation to refocus the a word that has been lured out of what might be seen as its native lexical habitat.  The difference is perhaps a matter of the nature of the underlying conceptual schemes implicated specifically in the interpretation of each of these distinctly non-literal phenomena, with the type shifting of coercion lending itself more readily to a hierarchical scheme of conceptual classes.  One way of putting this would be that, where the interpretation of metaphor seems to involve determining a context in which the conceptual deracination of a word's denotation can be understood in terms of an intensional transfer between domains, coercion interpretation comes across as more of a determination of the level of abstraction on which to consider the relationship between, for instance, a predicate and an argument.  Among other things, data involving scoring rather than just classifying potential instances of coercion would offer insight not only the operation of my methodology but also the way that humans cognitively approach this phenomenon.

XXX ANALOGY

Analogy stands out as something of a special case in the research presented here, in that, among other things, the geometric mechanism targeting analogy completion is entirely unsupervised: the hypothesis tested is that analogies should simply play out as predictable geometric configurations in an appropriately contextualised subspace.  The geometry specified, in line with other models, is that of the parallelogram, suggesting a conceptual coordination in terms of orientation and distance in a semantic space.  An analysis of optimal subspaces, though, indicates that there might be even more specific types of polygons implicated in the mapping of analogy: rectangles or squares with a certain orientation in relation to the edges of a positively valued subspace, for instance, might be even more strongly associated with the intensional coordination at play in analogy.  Choosing dimensions that are collectively best for facilitating these properties poses a potentially hard computational problem, however, as the angles and balances of side lengths of polygons are products of the overall situation of the shapes' vertices and cannot be even approximated based on a dimension-by-dimension analysis.  This in turn opens dimension selection processes up to 

\section{For the Future}
The work described in this thesis has involved the extrapolation of contextual semantic models from digitised textual data through the application of information processing procedures interacting at many different stages along the passage from raw data to semantic information.  This methodology can be understood in terms of the parameters which correspond to these stages, which can be summarised as follows:

\begin{list}
\item[Data] The corpus selected for building a base space of co-occurrence statistics and the data cleaning procedures applied to this corpus;

\item[Word Counting] The method used for tabulating co-occurrence counts, which in the work described here has involved simply seeing how close words are to one another in a sentence but could also be calculated in terms of, for instance, distance along the branches of a dependency tree;

\item[Statistical Processing] The function applied to word frequencies and co-occurrence counts in order to derive the elements of a base space for subsequent contextual projection;

\item[Subspace Selection] The techniques, including the choice of contextualising input, for applying a statistical analysis of word-vectors to the selection of a set of dimensions for subsequent geometric semantic analysis;

\item[Geometry] The measures and mappings used to extrapolate semantic information from the situation of word-vectors in a contextualised subspace.
\end{list}

\nodindent The research presented here has offered a sampling of possible combinations of these parameters; my intent has been to focus on higher level issues of theoretical grounding and philosophical import, in the context of a rigorous but also broad survey of my methodology's application to a handful of established natural language processing tasks.  This means that, to the extent that the results returned on various datasets are good enough to motivate future research with context sensitive distributional semantic models, there is already a considerable amount of work that can be done simply by way of exploring the parameter space of the methodology as it stands.  So for instance the type of language found in Wikipedia, which has particular, arguably even peculiar characteristics in terms of vocabulary, idiomaticity (or lack thereof), sentence length, and so forth may not be best suited for building dimensions that are amenable to contextualisation.  Likewise there might be more effective techniques for subspace selection, involving for instance an analysis of the relationship between word-vectors along a dimension, or even just an analysis of general statistical characteristic of dimensions such as mean and variance.

A particularly interesting topic for future research is the question of how best to translate word-counts into informative statistics about co-occurrence relationships.  The application of an information theoretical metric is attested in the literature and has proven fairly effective for the tasks described here, affording a mechanism for translating probabilities into geometric features, but there are a variety of other functions that can be applied to word-counts, as well.  Should logarithmic dimensions, for instance, be weighted by the probability of a joint occurrence of their correlates, allowing for a sort of entropic analysis of a dimension's distribution?  And does the warping factor of the smoothing constant and the shifting of ratios of probabilities prior to taking logarithms perhaps have an adverse effect on conceptual geometry?  The removal of these constants, which were introduced to enable the dimension selection process, may allow word-vectors to fall into an even more semantically interpretable geometry.  And if dimensions are constructed as probability distributions, then this opens them up to a potentially productive analysis in terms of probabilistic moments, providing another set of features that can possibly be mapped to the semantic character of subspaces.  There is even scope for applying a kind of reverse subspace selection procedure, seeking to induce, given a subspace which we know to be in some sense semantically productive, the kind of word-vectors which would indicate such a subspace.

In terms of analysing the geometry of subspaces, it may be worthwhile to consider any of these phenomena in terms of proximity by way of relative ranking within subspaces, rather than in terms of absolute geometric measures.  So, for instance, the similarity between pairs of words may be more accurately mapped as a function of the number of other word-vectors closer to either input word-vector, within a contextualised subspace, than either word is to the other: this could go some way towards addressing the issues of framing raised in Chapter~\ref{sec:frames}.  A similar measure could be applied to words' relative distance, in terms of rank rather than norm, from the origin.  And likewise, ranking measures could be applied to generic points of the space; in fact, this relativistic approach might serve to mitigate the influence of outliers in terms of maximum and mean vectors, lessening the impact of values that are simply aberrantly far from anything else by virtue of low independent frequencies combined with high joint frequencies.

More ambitiously, we might consider other ways of modelling the lattice of contextual subspaces itself.  The projections afforded by one of the base spaces that my methodology generates can, as outlined in Chapter~\ref{sec:litdims}, be understood as a power set of contexts, and so a discreet structure that can be thought of as an unweighted network of possible subspaces.  If we consider the relationships between dimensions, however, and in particular establish a metric of distance between dimensions, then the immense space of possible projections becomes a smooth manifold, a continuous space of at least suppositional positions within a lattice of infinite spaces.  This construct would have powerful mathematical attributes, potentially allowing for the modelling of semantic relationships in terms of differentiable movements across contexts rather than just as static relationships within dynamically selected but discreet subspaces.


Finally, the nature of the tasks towards which my methodology can be applied, and the corresponding evaluation of performance on these tasks, deserves further consideration.  The type of datasets used here, which 


This could give the subspace selection process a 

HUMANITIES

\section{Direct Encounters with Meaning}

I don't intend to suggest that the nature of 

If the phenomenal experience of reality -- the quality of the perception of a world of objects and processes -- is, as \cite{Clark} puts it, \emph{controlled hallucination}, the words too are a layer of the 

Word frequency is not a thing that is directly perceived: though I could make an informed guess, I do not actually have a sense of how often I've encountered the word \emph{the} recently, versus the word \emph{thundercloud}, versus the word \emph{fulgurate}.  Meaning, on the other hand, if we take XXX seriously, is directly perceived.

``the familiar physiognomy of a word, the feeling that it has taken up its meaning into itself,'' \citep[][p. 218]{Wittgenstein}

What I am ultimately seeking to set up here is the groundwork for the application of computational techniques to phenomenologically oriented cognitive models.  This is why I've attempted to make my methodology conversant with, for instance, \cite{Davidson1978} theory of metaphor, which is ultimately about the way that language gets outside of the portage of propositions about situations in the world and into the actual fabric of the experience of existing.
