\chapter{Background}
The ambitious scope of this project requires a thorough grounding in the literature from a range of disciplines.  The risk in such an inclusive undertaking is that there are many opportunities for going astray.  The reward, a piece of empirical work that exemplifies the possibility of using computers to do theoretically significant research, seems to justify the risk, but only if the final product is well placed in the context of existing work in the area.  The following chapter will delineate the territory that this thesis seeks to explore, by way of a survey of the considerable canon of theoretical work relating to the problems of minds, concepts, and language as well as the increasingly substantial body of practical work on computational linguistics and its application to the modelling of creative language generation.

The theoretical background for my project in particular will lead to the development of an inventory of what \cite{Gallie1956} has called \emph{essentially contested concepts}, words and corresponding ideas that are more likely to invite debate and academic dissent than to offer resolution.  As \cite{Deacon} has put it in his biologically grounded account of the emergence of goal-directed behaviour, ``Such concepts as information, function, purpose, meaning, intention, significance, consciousness, and value are intrinsically defined by their fundamental incompleteness,'' (ibid, p. 23).  But, as Gallie points out in the context of the social sciences, these words are nonetheless important and can be useful components of a productive discourse, just so long as we are not overambitious in our claims to have arrived at some sort of conclusion about their objective definitions.  Instead, I propose that the ideas of \emph{information}, \emph{meaning}, \emph{creativity}, \emph{representations}, and \emph{concepts} should be viewed as boundary conditions for the empirical work that will be the primary focus of this thesis, delineating the 

\section{Meaning Making}
At its heart, this thesis is about the emergence of meaning from data, and in this regard it sits atop a tradition of analytic enquiry into the nature of being itself.  The very question of how meaningfulness can come about in a material universe has been arguably the unifying theme of modern Western Philosophy, spanning from the \emph{cogito} of \cite{Descartes1911} to the phenomenology of \cite{Husserl1900} and \cite{Heidegger1926}, by way of empiricism \citep{Locke1689,Hume1738}, transcendental idealism \citep{Kant1787}, pure idealism \citep{Hegel1816}, and intentionality \citep{Brentano1874}, to delineate just one of the countless pathways through the rich tradition of ideas about minds.  Broadly speaking, I intend to present a philosophically motivated, empirically oriented project that, without making controversial commitments or overambitious overtures, sits comfortably with \citepos{Wittgenstein1953} idea that ``only the act of meaning can anticipate reality,'' (ibid, \P 188), which I will interpret to suggest that meaning is somehow properly in the world, not only in some immaterial, nominally mental space---but also that there really is such a thing as meaning, that it is not merely a convenient fiction of an otherwise behaviouralist ontology.

With this in mind, the project I describe here is broadly conversant with \citepos{Floridi2011} pursuit of a \emph{theory of strongly semantic information}, by way of which he arrives at a quantitative model of meaning.\footnote{Unlike \cite{Fredkin2003} and, more popularly, \cite{Bostrom2014}, Floridi navigates a middle way towards a computational model of semantics without committing to outright digital ontology.}  The idea that observable data can be computational transformed into information is underwritten by the Information Theory of \cite{ShannonEA1949}, which seeks without making any philosophical claims about knowledge or beliefs to formalise the measurement of what can be known in terms of the unexpectedness associated with sets of observations \citp[see][for a thorough treatment]{Pierce1980}.  An early attempt to import technical insight from signal processing into the study of meaning can be found in \citepos{CarnapEA1952}, who use Shannon-type metrics as the basis for quantifying the inferential properties associated with the semantic content of sentences, followed by \cite{Dretske1981}, who describes the formation of meaningful concepts in terms of the development of internal semantic structures that evolve to indicatively correspond with quantifiable informational situations in an environment.  Subsequent forays in a \emph{situation logic} designed to model semantic information content in a way which is simultaneously measurable and context specific \citep{BarwiseEA1983} have contributed to the resolution of computationally amendable formalisms, both in the tradition of Shannon and the semantics that have followed from \cite{Montague1974}, with the environmentally grounded approach to cognition which will be discussed presently.

At the more ambitious extent of the spectrum, the likes of \cite{Koch2004} and \cite{Tononi2008} have put forward theories attempting to quantify consciousness itself, generally in terms of the differentiable components of complex dynamic systems.  \emph{Consciousness}, however, is one of the aforementioned essentially contested terms, so instead of taking a stance here, I will take the easier route of simply acknowledging that there is a \emph{hard problem} to be solved, to use the jargon of \cite{Chalmers1996}, and it should be perfectly possible to do good empirical work without necessarily taking sides in the fraught debate over the computability of the subjective experience of existence---or rather, perhaps an effective empirical approach comes about precisely from recognising the intractability of the debate in the first place.  So here I will propose to use the notion of \emph{creativity} as a kind of representative for the entire idea that being a cognitive agent has something to do with the production of meaning in reaction to the rampant stimulus provided by a dynamic and unpredictable cognitive \emph{umwelt} \citep{VonUexkull1957}.  In the spirit of \cite{Koestler1964}, then, and his model of creativity as ``a new synthesis of previously unconnected matrices of thought,'' (ibid, p. 182), I will offer a general definition of creativity as the act of meaning making in a universe of heterogeneous environmental data, and I will further assert that modelling this type of cognitive activity is, in a general sense, the target of my research.\footnote{Creativity is itself, as \cite{Colton} has pointed out specifically in the context of computational approaches, an essentially contested concept, but, in the spirit of \cite{Gallie1956}, I will presume that there is significant value in identifying creativity as a boundary condition of sorts for the range of activities that I wish to explore without reaching a conclusive definition of the concept.}

This then pushes my research into the broad domain of \emph{computational creativity}, a field outlined in the seminal work of \cite{Boden1990} and subsequently formalised in terms of ``behaviour  exhibited  by  natural  and artificial systems, which would be deemed creative if exhibited by humans,'' \citep[][p. 206]{Wiggins2006b}.  The thrust of this work and the theory and practice that have sprung up around it involves treating creativity in terms of state spaces of combinatory components susceptible in the most productive cases to transformational transgressions of the rules for traversing the space, resulting in artefacts (and, arguably, processes) which can be evaluated in terms of their novelty and value \citp[see][among others for interesting theoretical work on the evaluation of computational creativity]{Ritchie2007,Colton2008,Jordanus2012}.  If meaning making is to be construed in terms of creativity, and creativity is in turn modelled as a process of combination and composition, then at the root of the computational application of a theory of data, information, and meaning we encounter another essentially contested concept, namely, that of \emph{representation}.

Representations have played a roll in philosophy of mind certainly since \cite{Descartes1911} and \cite{Hobbes1651}, and by any but the most abstracted interpretation at least since \cite{Plato1892}---perhaps they are a necessary passage in any movement towards a robust theory of mind \citep[if, in fact, such a theory is even desirable---\emph{cf}][]{Rorty1979}.  The recent trend in philosophy, however, not to mention in empirically fastidious fields such as cognitive science and psychology, has been towards a resolute materialist reductionism, to such an extent that \cite{Rowlands2010} reports that in the current cognitive scientific milieu, ``even the word `Cartesian' is often used as a term of abuse,'' (ibid, p. 12).  This has been bad news for representations which, when applied to a theory of mind, can degrade into a homuncular regression that \cite{Dennett1991} has described as the \emph{Cartesian theatre}: if something is being represented, and something is doing the representing, who or what is at the receiving end of the process?  The embodied and enactivist school of thought instigated by \cite{MaturanaEA1987} and pursued by, for instance, \cite{Haugeland1993} and \cite{Thompson2007}, has led to the reanimation of discourse regarding the nature of mind from a perspective that does not take the \emph{explanatory gap} \citep{Levine1983} between what is subjectively experienced and what is objectively described for granted.  Subsequently \cite{VanGelder1995} has outlined the premise of a mathematically tractable model of non-representational cognitive systems described in terms of dynamically coupled differential equations, while the emergentist system theory of biosemioticians like \cite{Kauffman1995}, \cite{Hoffmeyer1997}, and \cite{Pattee2001} have provided fertile material for the sophisticated and evolutionarily plausible cognitive model of \cite{Deacon2011}.

But these anti- or post-representationalist approaches to cognition tend to unravel a bit when it comes to saying anything about language.  In this particularly well travelled domain, the type theory of \cite{WhiteheadEA1927} and \cite{Church1940} still holds a certain sway, with the subsequent formalisms of intensional semantics \citep{FoxEA2005} treating language as an ineluctably symbolic phenomenon.  As such, there is an overt representationalism that is more or less necessarily at play in the symbolic commitments made by any sustainable theory of semantics, particularly in the context of natural language.  Regardless of whether the representations in question are strictly in the mind, a theory promoted by \cite{Fodor2001}, or are in some sense in the world in line with the philosophy of \cite{Putnam1975}, it becomes difficult to imagine an operational model of semantics which doesn't fall back on structures which are to some extent extracted from the reality that they denote.

\cite{McGregorEA2014} have presented something of a start towards addressing or, perhaps more to the point, avoiding this issue (and the issue has been subsequently explored by \cite{Coeckelbergh2016}, in both cases specifically with reference to computational creativity).  The idea put forward there is that, in the context of computational creativity in particular, it should be acceptable to take seriously the evident efficacy of talking about representations when talking about cognitive processes without necessarily making a commitment to the fundamental reality of such representations.  I will stick to this position in the work presented in this thesis: by starting with the assumption that representations are a useful, maybe even necessary, component when talking about semantics and meaning, I maintain that we might eventually arrive at a more satisfying resolution of why this kind of structure has held such sway over the modern Western tradition of analytic philosophy in particular, and whether this influence is fundamental or just incidental.  I don't claim to come close to actually answering this hard question, but I do think that there will be apparent merit in taking my methodology seriously as an empirical tool for gaining some sort of theoretical traction in this regard.  So, in summary, in the following chapters, I will be describing a methodology which traffics in a particular theoretically motivated variety of meaning bearing representation, without making any commitment as to the essentialism of that device; the desideratum of these representations is that they be susceptible to the environmental situatedness that is clearly an important component of any effective cognitive or linguistic model.  My contention here is that sound theoretical grounding based on insight from cognitive science should grant my models a degree of at least temporary immunity from accusations of dualism.

\section{Concepts}
As \cite{Searle1983b} points out, representations have intentional content: they have to be about things, whether or not they take the form of materially or abstractly transportable entities like words or icons.  The intentionality of representations invites the addition of another term to our growing catalogue of essentially contestable concepts, this time the word \emph{concept} itself, which I will take to refer to the cognitive aspects of the things indicated by representations.  The idea that concepts are interactive structures of the mind \citep{MargolisEA2007,Fodor2008} has been productive in aligning cognitive science with computational modelling \citep{Boden2006}.  If concepts can be modelled as rule bound composite symbolic entities, then a symbol manipulating, constraint satisfying device should provide the right kind of architecture for simulating productive interactions between conceptual representations.  This type of modelling has proven practically effective in, for instance, the structured ontology of \cite{Lenat1995} and the graph theoretical work of \cite{Sowa2006}.

There is discord afoot, however, amongst researchers interested in modelling concepts, parallel to a certain extent to the debate over mental representations outline in the previous section.  The net result of this tension has been the generation of a kind of negative space: where philosophers like \cite{FodorEA1988} have made a convincing case against treating concepts as associationist networks, more recent cognitive scientific research from the likes of \cite{Hutto2001} and \cite{Chemero2009} offers a likewise compelling rebuke to any theory of mind that falls back on a framework of symbolic conceptual representations.  What remains is a clearly developed picture of what cannot constitute a concept in a cognitive model, but a much more murky impression of what positively does count as a thought or a perception and so forth.  A remedy of sorts is offered by \cite{Gibson1979}, with his view of cognition in terms of the direct perception of environmental \emph{affordances} of opportunities for action in a situation.  \cite{Clark1997} has expanded upon this to arrive at a notion of \emph{action-oriented representations} which outsource much of the computational load of conceptualisation to the physical and spatial domain of a cognitive agent's environment.

Here \cite{Kant} has proved to be, perhaps not surprisingly, especially profound: the Kantian notion of a domain of \emph{conception} that is supervenient upon an underlying field of emph{intuition} which is in turn grounded in the essentially geometric nature of reality provides a philosophically robust starting point for a spatial model of conceptualisation.  By positioning conceptual models geometrically, the components of concepts which give them the composability that symbolic models afford while at the same time maintaining some degree of contact with the potentially physical context of space.  The work of \cite{Gardenfors2000} is particularly germane here, and will serve as a primary point of reference for the methodology that I present in this thesis.  By modelling concepts in terms of convex regions within conceptual spaces defined by interpretable dimensions representing attributes of the concepts themselves, G\"{a}rdenfors provides a plausible intermediary between the low-level stimulus to which a cognitive agent is exposed in an environment and the high-level symbols that become the representational currency of thought and communication: stimuli provide the data which becomes the values defining the points in a symbolically realisable conceptual space.  More recent work has explored the way that a conceptual space model can be applied to lexical semantics in order to provide a geometric grounding for the categorical nature of language composition \citep{Gardenfors2016}.

The environmental grounding of a conceptual model further provides a mechanism for understanding the important role of \emph{context} in cognition.  Here \citepos{Barsalou1992} work modelling concepts in terms of \emph{frames} offers a valuable perspective on the way that particular conceptual schemes are activated in response to situations in the world.  Barsalou's approach facilitates notions of prototypicality and periphery that emerge in the course of online, context sensitive conceptualisation, once again at least hinting at a spatial component of this cognitive framework.  Also of note is the \emph{conceptual blending} approach of \cite{FauconnierEA1998}, which makes use of a spatial theory of mind to develop a framework of conceptualisation as integration between frames of representation.  This approach has been applied in the domain of computational creativity in particular, to the generation of language in the case of \cite{Veale2012b} and to automatic software generation by \cite{ZnidarsicEA2016}.  And it is also worth mentioning the \emph{global workspace} framework proposed by \cite{Baars1988}, which models cognition as a multi-agent system in which functional components compete and collaborate to forge a situated cognitive gestalt: this approach has been adopted by \cite{Shanahan2010} in his work on cognitive robotics and by \cite{Wiggins} again in the domain of computational creativity.  A common and significant theme here is the dynamism and distribution inherent in all these approaches, contravening conceptual models that resort to static and hierarchical representational regimes.

Ultimately, I think we have to take seriously \citepos{Davidson1974} case against the idea of conceptual models in the first place.  Davidson's point is not so much that there is no such thing as a concept -- that would be a fatuous claim -- as that concepts are an artefact of the way that cognitive, and in particular linguistic, agents use meaning bearing representations to structure thought and communicate about experience.  At first glance this view of concepts might appear as facile as the denial of the existence of concepts is fatuous: obviously concepts have something to do with having thoughts, and it is probably impossible and certainly pointless to imagine a universe in which there are concepts but there are not cognitive agents.  But the subtlety of Davidson's point is that there is a dynamic between conceptual models and representational structures which belies any kind of relationship of supervenience and complicates attempts to explain cognition in terms of levels of materialistic abstraction---as, in their own distinguished and insightful ways, \cite{Floridi2011} and \cite{Deacon2011} have each done.  This dynamic turn invites a consideration of language as a concept supporting structure, and so sets us up for the next section of this survey of the established theory and practice surrounding my own work.

What we are then left with is the impetus for a computational approach which should be situationally dynamic and contextually sensitive.  With this in mind, the methodology that is the focus of this thesis will be characterised by semantic representations that are designed to be understood as conceptually productive, contextually generated perspectives on spaces defined in terms of statistical data about language use.  By using quantitative data to project representations into spaces that can be manipulated in an open ended way in response to a context which in principle can be arbitrarily defined, I will seek to mirror a theory of situated cognition permitting for the emergence of concepts in the course of the dynamics between agent and environment.  As with my treatment of semantic representations themselves, I don't claim to be describing a methodology for conceptual modelling which is necessarily plausible on the level of physical or biological processes; instead I take certain assumptions about conceptual spaces for granted, and so there is an element of abstraction necessarily at play here.  Once again, though, my stance is that allowing for some \emph{a priori} assumptions about what is conceptually permissible provides a sound basis for getting on with the practical work of designing data driven experiments based on conceptual models and then turning around to apply the experimental outcomes to a productive reconsideration of theoretical assumptions.

\section{Words}
What has come to be known as the Cognitive Revolution finds its origin in, among other things, \citepos{Chomsky1959} pointed denouncement of \citepos{Skinner1957} attempt to apply psychological behaviouralism to the study of language.  Chomsky's point is that language can only properly be understood as a specialised faculty that is in some way, more than just a mode of stimulus and response, internal to the cognition of a linguistic agent: in order to effectively model language, we have to build some sort of notion of minds populated by cognitive content and attendant intentionality into the equation.  For Chomsky and some of his acolytes, the logical extension of this view has been the development of a programme founded on the idea that language is itself an inborn characteristic peculiar to human cognition, certainly neurologically specific and quite possibly genetically encoded \citep{Chomsky1986, Pinker1994, Fodor2001}.  A significant component of this project has been the development of various formulations designed to systematically encapsulate the conditions generally determining the parameters of natural languages, but for every attempt to categorically describe the particulars of human communication, linguistic anthropologists such as \cite{Levinson2001} and \cite{Everett2005} turn around and discover a group of language users who provide the exception which in the case of a scientific approach to language really does disprove the rule.

The movement against Chomskyan nativism has tended to swing towards what is arguably an even more fundamentally cognitive theory of language, often characterised by interpretations of \cite{Sapir1970} and \cite{Whorf2012} as a jointly declaring that language is, to a greater or lesser extent, actually the foundation upon which thought and attendant cultural spheres are built.  More generally, the field of cognitive linguistics has emerged in response to the mainstream linguistic stance supporting theories of universal grammars, and a battery of interrelated linguistic models have emerged from the idea that language is, along with various other aspects of human behaviour, broadly wrapped up in and symptomatic of the general condition of having a mind rather than a compartmentalised cognitive faculty \citep{CroftEA2004}.  Of particular relevance here is the \emph{cognitive grammar} of \cite{Langacker}, which proposes to overcome the divide between syntax and semantics by treating phonological and morphemic components of language as inextricably intertwined with semantics in ways that supersede evident distinctions across what Langacker calls \emph{grammatical classes} (conventionally, parts of speech, basically).  Also of note are the \emph{image schema} of \cite{Lakoff1987} and \cite{Johnson1990}, who, by focussing their analysis on the way that preposition usage in particular suggests distinct culturally specific embodied models of the world, developed environmentally and biologically grounded frameworks for productive semantic composition.

%It's also worth mentioning the \emph{neural language theory} of \cite{Feldman}, not least because it seeks to apply a computational model to a likewise schematic semantic formalism, though that work is based on the presumption that the brain and mind are both computational

A general methodological commitment of cognitive linguistics is the qualitative analysis of instances of language use applied to the development of critically rich models of how conceptual and linguistic representations interface in the course of situated cognition.  It should not be presumed, however, that cognitive linguists take semantic and conceptual representations to be identical or even isomorphic, and in fact \cite{Evans2009} argues specifically that it is the nebulousness of the relationship between these domains that gives language its particular qualities of looseness and ambiguity by which lexical representation can be deployed in context specific ways to achieve an open-ended expressivity.  This aspect of semantics is particularly evident in the phenomenon of figurative language, and the study of metaphor has been an especially successful pursuit here, with a valuable compendium of the productive era from the late 1970s through the 1980s assembled by \cite{Ortony}.  Exemplary theoretical work grounding the seemingly unlimited generative capacity of figurative language in a robustly cognitive approach to linguistics includes the \emph{interaction} view of \cite{Black1955,Black1977} and the \emph{reconstructivist} stance of \cite{Ortony1975}.  It is the \emph{cognitive metaphor} approach of \cite{LakoffEA1980}, however, which stands out most of all here, not least because it has provided the most consequential material for latter day computational research into metaphor classification and interpretation \citep{Shutova2015}.  The description of metaphor in terms of isomorphic mappings between conceptual domains lends itself to precisely the type of symbolic manipulation of information structures that have characterised traditional AI, and, as it turns out, can also provide a theoretical grounding for sophisticated statistical modelling of lexical semantics \citep{ShutovaEA2013}.

Statistical approaches to lexical semantic modelling will be surveyed in more detail in the following section, but a brief overview of information processing applications of the theory surrounding metaphor seems appropriate here.  Some early computational approaches to metaphor maintained an essentially formal character: \cite{vanGenabith2001} proposed a type theoretical model for describing metaphor.  Information processing approaches have, though, been by and large data-driven, understandably utilising the processing power of symbol manipulating machines---and these data-driven approaches have generally had some sort of connection with the cognitive linguistic stances on metaphor.  So, for instance, \cite{ThomasEA1999} describe an information processing network which selectively projects features, inspired by the previously mentioned interaction view of metaphor developed by \cite{Black1977}.  In terms of theoretical grounding, \cite{Shutova2010} identifies the \emph{selectional preference violation} approach of \cite{Wilks1978} as especially influential, perhaps because it was formulated specifically as an information processing mechanism.  A notable early effort from \cite{Fass1991} is derived from this theoretical background, with correspondences in the selectional preference of the arguments of verbs used to detect metonymic versus metaphoric uses of language.

The mainstream of metaphor modelling has subsequently been characterised by symbol manipulating approach and, in the spirit of the conceptual metaphor model, has  involved mapping between conceptual schemes \citep{Indurkhya1997}, often domain specific, with the underlying assumption that mappings between domains correlates with the conceptual metaphor model \citep{Narayanan1999}.  Typical symbolic approaches to metaphor modelling involve the construction of an ontology defined by features which can be mapped between elements.  The ATT-Meta system \citep{LeeEA2001}, with its faculty for backchaining inferences across conceptual domains, is exemplary, and has furthermore been expanded into a metaphor generating system employing a combination of distributional semantic and incremental grammar techniques \citep{GargettEA2013}.  Other symbolic approaches are notable for their recourse to pre-formulated knowledge bases such as WordNet \citep{VealeEA2015}, or the web at large in conjunction with other resources \citep{VealeEA2007}.

Symbolic approaches have tended to focus on the interpretation of metaphor by way of models of trans-conceptual mappings, but in another aspect of computational work, that of metaphor identification, statistical approaches have proved particularly effective.\footnote{\cite{Shutova2013} suggests that computational identification and interpretation of metaphor, in line with psychological analysis, should be considered a joint task.}  An early example is the TroFi model of \cite{BirkeEA2006}, which uses a clustering algorithm trained on a set of tagged sample sentences to disambiguate between literal and non-literal verb use, followed by \cite{Utsumi2011}, who explores clustering in the context of distributional semantics.  Indeed, many of these statistical approaches (see \citealt{TurneyEA2011}, \citealt{Dunn2013} for a comparison of distributional semantic and symbolic models, \citealt{ShutovaEA2013} for an overview of statistical models in particular) have employed the techniques of distributional semantics, which will be discussed in the next section: here \citepos{Kintsch2000} model of metaphoric interpretation as a contextually selective traversal of the space between word-vectors is seminal.  A notable recent instance of a statistical model for metaphor identification involving an application of compositional distributional semantics is described by \cite{GutierrezEA2016}, of particular note here as the dataset presented by those authors will be used to evaluate the model at the heart of this thesis (see Chapter~\ref{chap:figurative} for a more detailed description).  Returning to the cognitive linguistic foundations of computational approaches to metaphor, \cite{TsvetkovEA2014} go so far as to propose that their results derived from the statistical construction of what they construe as conceptual features associated with lexical representations ``support the hypothesis that metaphors are conceptual, rather than lexical, in nature,'' (ibid, p. 248).

There is another theoretical twist which must be mentioned here, however, and it comes once again from \cite{Davidson}, this time by way of his controversial claim that the meaning of metaphoric propositions should always be taken at face value.  Part of Davidson's point is that there is a pragmatic distinction to be drawn between what the metaphor means, which is to some extent in the language, and what the metaphor communicates, which is on the other hand in the world.\footnote{Davidson's account, which is famous or perhaps notorious amongst theoretical linguists, is notable in its absence from the computational literature, though it has recently been acknowledged at least in passing by \cite{Veale2016}.}  The presumption in both conventional semantic views of metaphor such as \citepos{Searle} as well as the more strongly cognitivist stances discussed above is that metaphor necessarily involves the projection of some aspect of meaning from one conceptual domain to another, but the point that Davidson raises is that there is a limit to the cognitive content that can be propositionally conveyed by language, and metaphor often reveals that limit.  To borrow a popular example from the discourse surrounding relevance theory \citep[][for example]{GibbsEA2006,Carston2012}, there is a lurking breakdown in interpretation when we try to apply any sort of transference view of metaphor to a statement such as ``my boss is a bulldozer'': presuming a small degree of contextual knowledge, we might easily understand that the speaker means the boss in question is inappropriately insensitive or aggressive in dealing with employees---but it is hardly clear what actual properties of \textsc{bulldozer} are transferred to \textsc{boss}, particularly in a situation which might very well not even be physical.

To address this issue, \cite{Carston2010} proposes that metaphor necessarily involves the generation of \emph{ad hoc concepts} that come about in the process of making a lexical mapping from one domain of encyclopaedic knowledge to another.  Drawing on \citepos{Barsalou1993} notion that language produces concepts in a way that is inherently \emph{flexible} and \emph{haphazard}, \emph{ad hoc} concepts offer a relevance theoretical account of the way in which language always pragmatically, situationally specifies the semantic content of an utterance \citep{SperberEA1995}.  This accommodates the \emph{deflationary} view of metaphor put forward by \cite{SperberEA2012}, which holds that metaphor merely occupies an especially inferential extent of a spectrum of meaning making and interpreting activities.  At stake here is the idea that language is not so much a system for codifying propositions about the world as a mechanism for achieving optimal communication of cognitive content, with the important proviso that cognition itself is primed for a perpetually unfolding contextualisation of the environmental stimuli available to an agent.  This ultimately means that metaphor is able to be more than just a highly efficient way of encoding propositions about concepts; it can, even in relatively mundane instances, extend itself into domains bordering on the phenomenological, a stance eloquently summed up by \cite{Reimer2001} in her apologetic exegesis of Davidson: ``For the goal of the metaphor-maker is not to get the hearer to see that something is the case, to grasp some deep and subtle truth, but to see something in a certain way, and seeing something in a certain way is simply not the sort of thing that can be given literal expression,'' (ibid, p. 150).

With all this in mind, we arrive at a further specification for the boundary conditions of our computational semantic model: in addition to being a representational system with a capacity for summoning context specific relationships between lexical semantic entities, it should also be able to generate new conceptual representations in an \emph{ad hoc} manner.  This implicates the modelling of conceptual spaces that are not merely invoked by the process of specification inherent in communication, but actually generated in the course of lexical dynamics.  And the situated, even arbitrary production of conceptual relationships in turn suggests, beyond just the activation of existing or implicit networks of association between semantically tractable entities, the online creation of entirely new connections and correspondingly of new ideas: put simply, the open-ended generation of conceptual spaces is the machinery of meaning-making.  It seems more or less impossible to imagine a regime of strictly symbolic representations which could fulfil these requirements, because symbols necessarily come with the logic and extent of their combinatory potentials, setting the constraints for the state space of their potential for interactive conceptualisation, more or less built in.  Instead, I propose that a statistical approach, in which lexical semantic representations are defined in terms of observations of symbols in use rather than rules applied directly to symbols, will offer the right kind of flexibility and dynamism for modelling the situated nature of concepts and the rampant looseness inherent in the relationship between words and objects of the mind.

\section{Data}
Finally, arriving at the technical background for the instantiation of the system of context sensitive, semantically productive representations outlined above, the research described in this thesis is grounded in recent and ongoing success in the paradigm of \emph{distributional semantics}.  The tradition of word-counting in order to predict sequences in language traces its roots back to the fastidious work of Andrei Markov, who tabulated co-occurrences of characters in Pushkin's \emph{Eugene Onegin} by hand \citep{BasharinEA2004}, and \cite{ShannonEA1949} propose a comparable application in their seminal work on information theory.  The idea of applying co-occurrence statistics to semantic applications is central to \citepos{Harris1954} work examining ``meaning as a function of distribution,'' (p. 155); the various consequent formulations of the \emph{distributional hypothesis} have been outlined by \cite{Sahlgren2008}, with \citepos{Pantel2005} asseveration that ``words that occur in the same contexts tend to have similar meaning,'' (ibid, p. 126) being representative.\footnote{Scholars frequently cite \citepos{Firth1957} quip ``you shall know a word by the company it keeps,'' (ibid, p. 179) as being foundational in the field.  I contend that Firth was referring in this passage specifically to the study of idiomaticity, particularly the way that idioms ossify culturally through repeated use, and this in the context of a larger proposal for a heterogeneous approach to the study of linguistics more in line with the comprehensive emergent view of \cite{MacWhinney1998} rather than anything that could be construed in terms of a computational, word-counting practice.  All the same, the quote has a nice ring to it and, taken out of context, serves its purpose.}  Theoretically speaking, computational linguists have ambitiously sought to ground distributional semantics in the formal semantics of Frege \citep{BaroniEA2014b} or indeed in the pragmatics of Wittgenstein \citep{GrefenstetteEA2011}.

Rather than indulge in speculation of what Wittgenstein might have done with a computer, I will propose a less likely candidate as the philosophical forbearer of word-counting as a productive applied linguistic practice: the semiotics of \cite{Peirce1932}, which maintain that the very physiognomy of meaning bearing structures, or \emph{signs} in Peirce's parlance, are semantically productive by way of their very physiognomy, and that they gain this productive structure through their ongoing contact with their environment.  From his own analysis of Peirce, \cite{Eco} extrapolates a notion of \emph{unlimited semiosis} by which signs participate in an infinite regression of semantic productivity, with one sign becoming the substrate for the constitution of a subsequent sign.  This begins to look, in an abstract way, a bit like the distributional semantic regime, where the sentential context in which words are found becomes the substance of interactive lexical semantic representations.  Another historical touchpoint is, as \cite{MillerEA1991} have pointed out, the \emph{salva veritate} of Leibniz, by which, in terms of logical formalisms, terms are considered to be synonymous if they can be universally interchanged in logical expressions without changing the truth values of the expressions.  Exporting this notion to the domain of computational linguistics, we arrive at the central dogma of distributional semantics, namely, that words can be modelled in terms of observations of their co-occurrence tendencies across large scale corpora, and furthermore that words with similar profiles can be interpreted as being likewise semantically associated.

Practically speaking, early work from, for instance, \cite{SaltonEA1975} suggested that the information content of documents could be effectively indexed by representing them as point in a vector space whose dimensions correspond to weighted measures of word frequency within a given document.  \cite{Schutze1992} extends this insight to represent words as vectors defined by the frequencies with which they are observed to co-occur with other words in a corpus, and uses angular measures from the consequent vector space as grounds for disambiguating the senses of polysemous words.  An important result of modelling words in terms of their co-occurrence profiles is that two words which have never been observed in proximity to one another might nonetheless turn out to be very close in the model and therefore very similar to each other: so, for instance, we can imagine a language in which the words for \textsc{cat} and \textsc{dog} are prohibited from ever being used in the same sentence, but we might still discover a semantic correspondence between the concepts because their signifiers tend to have similar patterns of usage.  The conversion of raw word counts into weighted statistics, perhaps most basically through the application of term-frequency, inverse-document-frequency type metrics \cite{SaltonEA1988} but more typically in more recent applications with information theoretical functions \cite{Turney2001}, has produced particularly productive co-occurence based lexical semantic representations.  The geometric efficacy of passing co-occurrence statistics through logarithmic functions will be discussed in Chapters~\ref{sec:math} and~\ref{sec:6XXX}.

The vector space approach to distributional semantics has subsequently evolved into a productive computational programme.  The distributional semantic methodology typically involves the selection of a corpus, the traversal of this corpus in order to tabulate the counts of co-occurrence terms within a certain proximity of target words (typically defined in terms of a window of $k$ words around each observation of a target word), the application of a weighting function to the resulting co-occurrence matrix, and the projection of the weighted vectors into a space (see \citealt{TurneyEA2010} and, more recently, \citealt{Clark2015} for comprehensive overviews).  \cite{BullinaraEA2012} have reported comparative results based on a variety of weighting schemes, most notably \emph{positive pointwise mutual information} (PPMI), an information theoretical metric designed to build sparse matrices capturing the most semantically salient co-occurrence features of word-vectors.  Where PPMI simply disregards co-occurrences that are observed at a frequency below the overall corpus average, \cite{LevyEA2015b} explore a slightly more subtle techniqe of shifting their co-occurrence statistics to avoid massively negative logarithms; a similar metric will be the basis for my own methodology.  The construction of distributional semantic models also often involves an additional step of dimensional reduction by way of, for instance, principal component analysis, with a particularly notable technique involving singular value decomposition described by \cite{DeerwesterEA1990}.



The evolution of high powered computers and the related advent of massive corpora of textual data has 



To chart a passage through the territory mapped throughout this chapter, then, statistics reflecting the co-occurrences of words in a large scale corpus will serve as the data substantiating the informational character of dynamic lexical semantic representations which, in their interactions, will be projected into conceptually interpretable spaces that are in turn reflective of the evidently representational character of meaning making.



Finally, the technical methodology of this project is grounded in recent and ongoing success in statistical approaches to language modelling.  The tradition of word-counting in order to predict sequences in language traces its roots back to the fastidious work of \cite{Markov}, who tabulated co-occurrences of characters in Pushkin's \emph{Eugene Onegin} by hand.  The research described in this thesis is situated within the paradigm of \emph{distributional semantics} which has its roots in \citepos{Harris1954} work examining ``meaning as a function of distribution,'' (p. 155).  The guiding principle of this area of research is the idea that ``words that occur in the same contexts tend to have similar meaning,'' \citep[][p. 126]{Pantel2005}.  Other early work in the field included 

who were generally confined to theoretical output due to historical constraints on computational power.

\footnote{I note, in passing, that scholars in the field often allude to \citepos{Firth1957} quip ``you shall know a word by the company it keeps,'' (p. --) as seminal to distributional semantics.  I suggest that this turn of speech has actually been taken out of the context of Firth's intent, which was to consider the role of idioms in particular in a comprehensive programme of linguistic study at various levels of abstraction, a project which is probably actually very much at odds with purely statistical approaches to language.}

Notwithstanding \citepos{Chomsky} formidable case that the bulk of a language cannot be captured in numbers quantifying the relative situation of words, the fact remains that, when it comes to computational approaches to language, a matrix capturing the nature of some concordance or another remains one of the main games in town.  Two post-millennial developments have facilitated the rapid development of research into computational methods employing word counting approaches: the accretion of large-scale, widely available digitalised textual data, and the advent of computers with the processing power to efficiently handle data of this scale.

which characterise a natural language could possibly be found in a matrix of statistics about the way that words co-occur with one another across some corpus, regardless of how comprehensive the data may be.

On the other hand, in as much as computational approaches to language modelling are concerned, co-occurrence statistics are the main game in town.

\section{OLD}
At its heart, this is a project about the potential for symbol manipulating machines to model creativity, and in particular the creative use of language.  Computational Creativity is a field that, as \cite{Wiggins2006b} puts it, uses ``computational means and methods'' to study ``behaviour exhibited by natural and artificial systems, which would be deemed creative if exhibited by humans,'' (p. 210).  At stake here is the problem of what creativity actually is, and, as \cite{Colton} have said, the concept of creativity must be \emph{essentially contested}.  Nonetheless, a basic position will be taken here that at the root of creativity is, in the spirit of \cite{Wittgenstein1953}, an act of meaning-making, by which a new way of conceptually representing something in the world comes about.

The prevalent take on computational creativity has focused on an AI approach involving the traversal of state spaces for potential creative artefacts, a methodology with deep roots in \cite{Boden1990}.  Philosophical problems with this essentially symbolic mode of generation have been illustrated by \cite{McGregorEA2014}, though: the dependency on preconditioned representations remits little in the way of actual creative behaviour on the part of the symbol manipulating agent.  Here the hard question of evaluation comes up, and the evaluative challenges peculiar to computational creativity have been addressed by \cite{Ritchie2007}, \cite{ColtonEA2012B}, and \cite{Jordanous2012}, to name a few.  At the core of this project is the proposal that, for an agent to be perceived as genuinely and autonomously instigating meaning-making, there has to be at least a nominal notion of the trafficking of dynamic, interactive representations within the agent's cognitive framework.  It is here that computational creativity will serve as a compass to keep this project on course: by returning to the question of whether the model outlined here can be described as an agent that is behaving creatively, the pragmatic issue of the role of contextualisation in the actual use of language will stay central to the thesis.

In addition to a general survey of the field, this project's particular commitment to conceptualisation and metaphor invites a survey of the appreciable work done in the computational production of metaphor and analogy.  With roots in the conceptual blending theory of \cite{Turner}, this area has been explored more recently by the likes of \cite{VealeEA2007} and \cite{O'Donoghue}.  Additional attention is due to work in poetry generation coming from \cite{Gervas}, \cite{Toivonen}, \cite{Rashel}, \cite{Cardosa}, and others.

\section{Minds and Spaces}
\cite{Davidson} cautions against considering language as an ``organising system'' (p. 11) for conceptual schemes.  In the process of understanding a sentence, a contextually specific theory of meaning is devised on the spot, so to speak, that permits ``an acceptable theory of belief'' (p. 18) regarding the sentence's author.  This means, though, that the conceptual organisation supposedly inherent in language is in fact a fleeting artefact that temporarily props up a momentary propositional stance, and language is just a mechanism for groping towards some sort of understanding between communicants regarding positions on the relationship between meaning and belief.  A consequence of this theoretical insight is the temptation to resort to a view such as that taken by \cite{Clark}, who holds that language is ``a cognition-enhancing animal-built structure,'' (p. 370).

The theoretical premise of this project is that a structural, spatial, geometric language model can successfully capture significant aspects of conceptualisation.  In this regard, the work is grounded in the theories of \cite{Gardenfors} and \cite{Widdows}.  G\"{a}rdenfors in particular postulates a kind of cognitive middle ground where conceptual spaces, characterised by coherent dimensions which map to features of concepts, stand between low level stimuli and high level symbolic representations.  Widdows, on the other hand, is more concerned with the description of a language model where a geometry of words facilitates the mechanisms of logic.  In both cases, these authors provide insight into the way that a specifically geometric view of representations allows for the dynamic interaction of symbols by way of using language as a cognitively productive system.

This project's commitment to building a system which remits evidence of dynamic representational structures is admittedly reminiscent of \cite{Fodor}, who holds that concepts are fundamentally compositional mental entities.  Any nativist forebodings engendered here are diffused somewhat by \citepos{Putnam} insight that conceptual extension cannot strictly be found ``in the head'' (p. 170).  In this regard, work in embodied cognitive science, rooted in the enactivism of \cite{Varela}, provides the cognitive foundation for a spatially charged theory of concepts.  So \cite{Thompson} goes on to offer a definition of information as ``the intentional relation of the system to its milieu,'' (p. 59), a view which attempts to capture the tight relationship between the spatial situation of the world and the symbolic representation of that situation \citep[][is another notable proponent of this approach]{Pattee}.  Similarly, the Heideggarian account of \cite{Wheeler}, with reference to \cite{Clark}, postulates an \emph{action-oriented representation} where the very structural nature of representations are ``deeply dependent on the specific context of action,'' (p. 196).  In the end the theory of language embraced here will be characterised by \citepos{Gibson} notion of \emph{affordances}: just as with physical objects, words present themselves as opportunities for communication, and it is in the ready-to-hand appropriation of meaning that the genesis of metaphor is to be discovered.

\section{Words and Concepts}
In the spirit of \cite{Peirce}, this thesis is predicated on the idea that the signs constituting a language are involved in a physically grounded process that is perpetually unfolding in tight entanglement with the world.  As mentioned in the previous section, this stance leads on to the view that the compositional components of a language offer themselves as affordances for communication in the context of a particular environmental situation.  It is by virtue of this opportunistic seizing of linguistic units as operational elements in a system of contextualised conceptual representation that language becomes so readily figurative.  And it is here the work connects with the theories of metaphor produced by \cite{Black} and \cite{Lakoff}, who see figurative language as interactions between isomorphic conceptual structures of one sort or another.

The account of metaphor as a simple process of intensional projection, such as that offered by \cite{Searle}, is confounded by \cite{Davidson}, who makes a controversial but ultimately persuasive case for the idea that so-called figurative language must in fact be interpreted literally because the pragmatically meaningful component of metaphor transpires on a level which is non-sentential.  \cite{Carston} fleshes out this claim, drawing on the inherently \emph{imagistic} quality of metaphor to offer an account of how the conceptual economy inherent in figurative expression involves components that cannot possibly be captured propositionally.  Similarly, \cite{Reimer} draws out a distinction between the \emph{meaning} and the \emph{intimation} of a metaphor, concluding that it is this second, no-propositional quality that gives figurative language its distinct character.

At stake here is a basic notion about the contextual nature of conceptualisation, something that \cite{Barsalou} has described in terms of the \emph{haphazard} way in which concepts are formed in response to a situation in an environment.  In the same vein, it is again \cite{Carston} who, from a relevance theoretical perspective, has characterised the formulation of concepts as \emph{ad hoc}, in particular analysing the way in which meaning is appropriated for some pragmatic purpose in the course of metaphor making.  In response to Carston, \cite{Allott} have made the case that \emph{ad hoc} concepts cannot possibly be \emph{atomic}, preferring instead to view concept formation as a process of inference involving some sort of clustering of information on a contextual basis.

This last theory of conceptualisation as a process of informational clustering begins to look a lot like the computational model that will be described in this thesis.  By pushing these clusters into an explicitly geometric representation, the hope is that a kind of non-propositional structure will emerge, satisfying apt points regarding the nature of metaphor raised by Davidson and his acolytes and, at the same time, providing a concrete basis for the sort of conceptually anatomic mapping proposed by Lakoff and Johnson.  Viewed in this light, computational linguistics, with its propensity for the construction of informational structures in a high-dimensional space, becomes the natural domain for a dynamically unfolding, context sensitive model of figurative language.

\section{Computers and Language}
At last, from a practical perspective, this work must be placed in relation to the ongoing work in computational linguistics, and in particular on the type of high dimensional, corpus based, generally unsupervised language modelling that will be at the core of the project presented here.  The idea that statistical word representations can be constructed in such a way as to be dynamically interactive is inherent in the work on compositionality done by, for instance, \cite{CoeckeEA2011} and \cite{GreffenstetteEA2011} \citep[see][for an overview]{MitchellEA2010}.  While the model described in this thesis does not target compositionality, the idea that linguistic representations can only capture conceptualisation by being somehow dynamically interactive is implicit in distributional semantic approaches to compositionality.

In terms of vector space models of distributional semantics in general, \cite{Clark2015} offers a good contemporary overview of work in the field.  The model described here, populated as it is by literal co-occurrence statistics rather than abstract values learned through iterations over a weighted network, distinguishes itself from the ongoing trend towards models with abstract dimensions, which can trace its roots back to the matrix factorisation based techniques developed by, for instance, \cite{DeerwesterEA1990} and \cite{BleiEA2003}.  More recently, the neural network based approaches of, for instance, \cite{BengioEA2003}, \cite{CollobertEA2008}, and \cite{MikolovEA2013} have achieved impressive results.  Mikolov et al in particular, along with \cite{PenningtonEA2014}, who present a model involving a hybrid of weighted networks and matrix factorisation, have achieved state-of-theart results on analogy completion tasks.  In the case of these models, however, the opacity of the scalars which constitute the word-spaces means there can be no hope of capturing the contextually informed theory of conceptualisation surveyed in the previous section.  Instead, to the degree that context is captured at all, it is found in the multiplicity of directions of movement offered by the dimensionality of the one irreducible space.  Ultimately, it seems that the analogy test sets which these systems target really just embodied a very specific type of figurative language, and subsequent systems have, by focusing on this particular objective, built this specificity into their own procedures.

Beyond analogy, ongoing work in taxonomy completion and, more generally, the inferential capacities of statistical language models is germane to the project described in this thesis.  To this end, work from \cite{CimianoEA2003} and \cite{SnowEA2006} provides a good exemplar of current directions in the field.  More recently, \cite{SantusEA2014} have presented an energy based model for determining hypernymy based on lexical statistics, while \cite{LevyEA2015} have offered a rebuttal to some of the ongoing work in the area, suggesting that a range of supervised learning language models actually do not recognise conceptual relationships between words, but rather simply model the probability of a term being used to describe a categorical \emph{prototype}.  In related work, \cite{SocherEA2013} have proposed a neural tensor model for completing knowledge bases, again with impressive results.

Ultimately there is a certain correlation between the project described in this thesis and the comprehensive objective of research from \cite{BaroniEA2010b}, who describe a generalised statistical language model designed to perform well on a wide array of language processing tasks.  Elsewhere, \citepos{BaroniEA2010} work on composing adjective-noun pairs into singular vector representations is also relevant, as the adjective, represented on its own as a matrix, might be taken as a contextualising mechanism.
